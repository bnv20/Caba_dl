{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05O_d01EF4aK"
   },
   "source": [
    "![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJFn9387KeE"
   },
   "source": [
    "## NLP, 텍스트 분석\n",
    "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\n",
    "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
    "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\n",
    "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
    "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
    " - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\n",
    " - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\n",
    " - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
    " - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
    " \n",
    "#### Text 분석 수행 프로세스\n",
    "- 텍스트 정규화\n",
    " - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
    "- 피처 벡터화 변환\n",
    " - Bag of Words : Count 기반, TF-IDF 기반\n",
    " - Word2Vec\n",
    "- ML 모델 수립 및 학습/예측/평가\n",
    "\n",
    "#### 텍스트 전처리 - 텍스트 정규화\n",
    "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
    "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
    "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
    " - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    " - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1614740772975,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "a8WINW-wETZH",
    "outputId": "85aa09e4-cc30-42c7-c5e9-1a1a5e45d272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1614740890871,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "H8IhYjPTFd-r",
    "outputId": "11a0e3a1-877b-40b4-93c3-f833e049c703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화(sent tokenize) : 마침표, 개행문자(\\n), 정규표현식\n",
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(sentences)\n",
    "print(type(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1614741158680,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "1gY7rmJvD5dW",
    "outputId": "9e5594a2-2102-4eb3-dff5-24ae268d74ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화(word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "print(type(words),len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1614741715154,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "aqQjF140E8KV",
    "outputId": "3f74a7ca-d7e4-4386-ea32-85e0b3eff83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해서 모든 단어를 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text) # 문장별 분리 토큰\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)\n",
    "print(type(word_tokens), len(word_tokens))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1614742024201,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zli4RcSnHdbD",
    "outputId": "75116589-3252-4935-c028-2d0a434436e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스톱워드 제거 : the, is, a, will와 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1614742208416,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "FAUm6nikIAIa",
    "outputId": "0e1b6432-a5d1-4064-d961-d0fea2ea73e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# NLTK english stopwords 갯수 확인\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1614742587642,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "rSE_VxExI2k1",
    "outputId": "44a97ecd-6b30-4a04-a6a2-49ffa7b37a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords 필터링을 통한 제거\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "  filtered_words = []\n",
    "  for word in sentence:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords:\n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "print(all_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1614743336098,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "o6D1fvVKKP-1",
    "outputId": "73a028ac-a49f-46d0-a7b1-eda228d84e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus aums amus\n",
      "fant fant\n"
     ]
    }
   ],
   "source": [
    "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
    "# Stemmer(LancasterStemmer)\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fancist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1614743257311,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zjLN9vwxMmBF",
    "outputId": "7dcf4623-5173-4477-cb3b-05c9a895c748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1614743401934,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "kePGySerLME9",
    "outputId": "a1f8a051-0286-4b0f-88ea-ee7c4b61c271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amuse aumses amuse\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtNVRIkBlTla"
   },
   "source": [
    "GPU vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9737,
     "status": "ok",
     "timestamp": 1614817435356,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zh0_dm0ukiiM",
    "outputId": "98516aa4-b78f-4004-dea6-29a4e39ee036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 7s 59ms/step - loss: 235.4992\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 242.3844\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 243.9395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model = Xception(weights = None,\n",
    "                 input_shape=(height,width,3),\n",
    "                 classes=num_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "x=np.random.random((num_samples,height,width,3))\n",
    "y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "model.fit(x,y,epochs=3,batch_size=16)\n",
    "model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1614817438600,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "4q2FMHd_ncQ0",
    "outputId": "126e18df-785d-4c06-a6b1-34dd0170fbad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:9초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40440,
     "status": "ok",
     "timestamp": 1614817585374,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "CSFC-eyjn2Fj",
    "outputId": "77649725-7830-4977-eca3-c8ab60964d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 17s 1s/step - loss: 233.7611\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 244.0772\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 242.1810\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  model = Xception(weights = None,\n",
    "                  input_shape=(height,width,3),\n",
    "                  classes=num_classes)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop')\n",
    "  x=np.random.random((num_samples,height,width,3))\n",
    "  y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "  model.fit(x,y,epochs=3,batch_size=16)\n",
    "  model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1614817590039,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "MPQI50EmoI1j",
    "outputId": "837eb25d-423a-4bc9-ea5a-aedd0599d4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:39초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 벡터화 : One-hot encoding\n",
    "Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.  \n",
    "단점 : 문맥 의미 반영 부족, 희소 행렬 문제.  \n",
    "BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.  \n",
    "피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화  \n",
    "카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여  \n",
    "TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등  \n",
    "파라미터  \n",
    "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외  \n",
    "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외  \n",
    "- max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "- stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "- n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "- analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
    "- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
    "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3,0,1],[0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO 형식\n",
    "from scipy import sparse\n",
    "data = np.array([3,1,2]) # 0이 아닌 데이터\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "sparse_coo = sparse.coo_matrix((data,(row_pos,col_pos)))\n",
    "print(sparse_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO vs CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "['A', 'B', 'C']\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 \n",
    "# BOW 인코딩한 수치 벡터로 전환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A':1,'B':2},{'B':3,'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "print(v.feature_names_)\n",
    "print(v.vocabulary_)\n",
    "print(v.transform({'C':4,'D':3}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print()\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \n",
    "# 보통 영어의 관사, 접속사, 한국어의 조사 등\n",
    "\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 20,\n",
       " 'is': 5,\n",
       " 'the': 13,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 21,\n",
       " 'is the': 6,\n",
       " 'the first': 14,\n",
       " 'first document': 4,\n",
       " 'second': 11,\n",
       " 'the second': 16,\n",
       " 'second document': 12,\n",
       " 'and': 0,\n",
       " 'third': 18,\n",
       " 'one': 10,\n",
       " 'and the': 1,\n",
       " 'the third': 17,\n",
       " 'third one': 19,\n",
       " 'is this': 7,\n",
       " 'this the': 22,\n",
       " 'last': 8,\n",
       " 'the last': 15,\n",
       " 'last document': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
    "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소  \n",
    "문서 d(document)와 단어 t 에 대해 다음과 같이 계산  \n",
    "\n",
    "tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
    "\n",
    "tf(d,t): term frequency. 특정한 단어의 빈도수  \n",
    "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수  \n",
    "n : 전체 문서의 수  \n",
    "df(t) : 단어  t 를 가진 문서의 수  \n",
    "idf(d,t)=log(n/(1+df(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"\"\"\n",
    "The Corpus of Contemporary American English (COCA) is the only large, \n",
    "genre-balanced corpus of American English. COCA is probably the most \n",
    "widely-used corpus of English, and it is related to many other corpora of \n",
    "English that we have created, which offer unparalleled insight into variation \n",
    "in English.\n",
    "\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'balanced', 'coca', 'contemporary', 'corpora', 'corpus', 'created', 'english', 'genre', 'insight', 'large', 'offer', 'probably', 'related', 'unparalleled', 'used', 'variation', 'widely']\n",
      "\n",
      "{'corpus': 5, 'contemporary': 3, 'american': 0, 'english': 7, 'coca': 2, 'large': 10, 'genre': 8, 'balanced': 1, 'probably': 12, 'widely': 17, 'used': 15, 'related': 13, 'corpora': 4, 'created': 6, 'offer': 11, 'unparalleled': 14, 'insight': 9, 'variation': 16}\n",
      "\n",
      "[[0.26726124 0.13363062 0.26726124 0.13363062 0.13363062 0.40089186\n",
      "  0.13363062 0.6681531  0.13363062 0.13363062 0.13363062 0.13363062\n",
      "  0.13363062 0.13363062 0.13363062 0.13363062 0.13363062 0.13363062]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "print(tfidv.get_feature_names())\n",
    "print()\n",
    "print(tfidv.vocabulary_)\n",
    "print()\n",
    "print(tfidv.transform(corpus).toarray())\n",
    "print(tfidv.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy 설치\n",
    "# Java 환경 세팅\n",
    "# JPype1-1.1.2-cp37-cp37m-win_amd64.whl 다운로드 후 C:\\Users\\admin 경로에 저장\n",
    "# (caba)C:\\Users\\admin>pip install JPype1-1.1.2-cp37-cp37m-win_amd64.whl\n",
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KoNLPy] https://mr-doosun.tistory.com/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석으로 문장을 단어로 분할\n",
    "okt = Okt()\n",
    "print(okt.morphs('단독입찰보다 복수입찰의 경우'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['항공기', '체계', '종합', '개발', '경험']\n",
      "['나', '프로젝트', '흥미진진']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
    "print(okt.nouns('나는 프로젝트를 하고있는데 흥미진진하고 재미있다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "# 원형\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True, join=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n",
      "['꽃', '파란', '하늘']\n"
     ]
    }
   ],
   "source": [
    "# 명사인 품사만 선별해 리스트에 담기\n",
    "morph =  okt.pos('아름다운 꽃과 파란 하늘')\n",
    "print(morph)\n",
    "noun_list = []\n",
    "for word, tag in morph:\n",
    "    if tag == 'Noun':\n",
    "        noun_list.append(word)\n",
    "print(noun_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 아래와 같이 분할하세요.  \n",
    "\n",
    "- 명사 '나는 오늘 방콕에 가고싶다.'\n",
    "- 원형 '나는 오늘 방콕에 갔다.'\n",
    "- 형태소 '친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.'\n",
    "- 형태소/태그' 나는 오늘도 장에 가고싶다.'\n",
    "- 정규화, 원형' 나는 오늘 장에 가고싶을깤ㅋㅋ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사\n",
      "['나', '오늘', '방콕']\n",
      "원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('방콕', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('.', 'Punctuation')]\n",
      "형태소\n",
      "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
      "형태소/태그\n",
      "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가다/Verb', './Punctuation']\n",
      "정규화, 원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "malist1 = okt.nouns('나는 오늘 방콕에 가고싶다.')\n",
    "malist2 = okt.pos('나는 오늘 방콕에 갔다.', norm=True, stem=True)\n",
    "malist3 = okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.')\n",
    "malist4 = okt.pos('나는 오늘도 장에 가고싶다.', norm=True, stem=True, join=True)\n",
    "malist5 = okt.pos('나는 오늘 장에 가고싶을깤ㅋㅋ?', norm=True, stem=True)\n",
    "print('명사')\n",
    "print(malist1)\n",
    "print('원형')\n",
    "print(malist2)\n",
    "print('형태소')\n",
    "print(malist3)\n",
    "print('형태소/태그')\n",
    "print(malist4)\n",
    "print('정규화, 원형')\n",
    "print(malist5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 분류\n",
    "- 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측\n",
    "- 텍스트를 피처 벡터화로 변환, 희소 행렬로 만들고 로지스틱 회귀를 이용해 분류 수행\n",
    "- Count 기반 과 TF-IDF 기반의 벡터화를 각각 적용, 성능 비교\n",
    "- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼파라미터 튜닝을 일괄적으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=0)\n",
    "news_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  1 15 ...  0  5  8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(news_data.target)\n",
    "a = pd.Series(news_data.target).unique()\n",
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\n\\nThis module contains two loaders. The first one,\\n:func:`sklearn.datasets.fetch_20newsgroups`,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\\nwith custom parameters so as to extract feature vectors.\\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\n\\n**Data Set Characteristics:**\\n\\n    =================   ==========\\n    Classes                     20\\n    Samples total            18846\\n    Dimensionality               1\\n    Features                  text\\n    =================   ==========\\n\\nUsage\\n~~~~~\\n\\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original `20 newsgroups website`_, extracts the archive contents\\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\\n:func:`sklearn.datasets.load_files` on either the training or\\ntesting set folder, or both of them::\\n\\n  >>> from sklearn.datasets import fetch_20newsgroups\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\n\\n  >>> from pprint import pprint\\n  >>> pprint(list(newsgroups_train.target_names))\\n  [\\'alt.atheism\\',\\n   \\'comp.graphics\\',\\n   \\'comp.os.ms-windows.misc\\',\\n   \\'comp.sys.ibm.pc.hardware\\',\\n   \\'comp.sys.mac.hardware\\',\\n   \\'comp.windows.x\\',\\n   \\'misc.forsale\\',\\n   \\'rec.autos\\',\\n   \\'rec.motorcycles\\',\\n   \\'rec.sport.baseball\\',\\n   \\'rec.sport.hockey\\',\\n   \\'sci.crypt\\',\\n   \\'sci.electronics\\',\\n   \\'sci.med\\',\\n   \\'sci.space\\',\\n   \\'soc.religion.christian\\',\\n   \\'talk.politics.guns\\',\\n   \\'talk.politics.mideast\\',\\n   \\'talk.politics.misc\\',\\n   \\'talk.religion.misc\\']\\n\\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\\nattribute is the integer index of the category::\\n\\n  >>> newsgroups_train.filenames.shape\\n  (11314,)\\n  >>> newsgroups_train.target.shape\\n  (11314,)\\n  >>> newsgroups_train.target[:10]\\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\n\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\n:func:`sklearn.datasets.fetch_20newsgroups` function::\\n\\n  >>> cats = [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\n\\n  >>> list(newsgroups_train.target_names)\\n  [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train.filenames.shape\\n  (1073,)\\n  >>> newsgroups_train.target.shape\\n  (1073,)\\n  >>> newsgroups_train.target[:10]\\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\n\\nConverting text to vectors\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\n``sklearn.feature_extraction.text`` as demonstrated in the following\\nexample that extract `TF-IDF`_ vectors of unigram tokens\\nfrom a subset of 20news::\\n\\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\\n  >>> categories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n  ...               \\'comp.graphics\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       categories=categories)\\n  >>> vectorizer = TfidfVectorizer()\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> vectors.shape\\n  (2034, 34118)\\n\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features)::\\n\\n  >>> vectors.nnz / float(vectors.shape[0])\\n  159.01327...\\n\\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \\nreturns ready-to-use token counts features instead of file names.\\n\\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\\n\\n\\nFiltering text for more realistic training\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren\\'t from this window of time.\\n\\nFor example, let\\'s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score::\\n\\n  >>> from sklearn.naive_bayes import MultinomialNB\\n  >>> from sklearn import metrics\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.88213...\\n\\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what\\'s going on inside this classifier?)\\n\\nLet\\'s take a look at what the most informative features are:\\n\\n  >>> import numpy as np\\n  >>> def show_top10(classifier, vectorizer, categories):\\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\\n  ...     for i, category in enumerate(categories):\\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n  ...\\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\\n  alt.atheism: edu it and in you that is of to the\\n  comp.graphics: edu in graphics it is for and of to the\\n  sci.space: edu it that is in and space to of the\\n  talk.religion.misc: not it you in is that and to of the\\n\\n\\nYou can now see many things that these features have overfit to:\\n\\n- Almost every group is distinguished by whether headers such as\\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\\n- Another significant feature involves whether the sender is affiliated with\\n  a university, as indicated either by their headers or their signature.\\n- The word \"article\" is a significant feature, based on how often people quote\\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\\n  wrote:\"\\n- Other features match the names and e-mail addresses of particular people who\\n  were posting at the time.\\n\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\n\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called **remove**, telling it what kinds of information to strip out\\nof each file. **remove** should be a tuple containing any subset of\\n``(\\'headers\\', \\'footers\\', \\'quotes\\')``, telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\n\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')\\n  0.77310...\\n\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\n\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                       categories=categories)\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.76995...\\n\\nSome other classifiers cope better with this harder version of the task. Try\\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\\nthe ``--filter`` option to compare the results.\\n\\n.. topic:: Recommendation\\n\\n  When evaluating text classifiers on the 20 Newsgroups data, you\\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\\n  setting ``remove=(\\'headers\\', \\'footers\\', \\'quotes\\')``. The F-score will be\\n  lower because it is more realistic.\\n\\n.. topic:: Examples\\n\\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\\n\\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 7532\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정규화\n",
    "# 뉴스그룹 기사내용을 제외하고 다른 정보 제거\n",
    "# 제목, 소속, 이메일 등 헤더와 푸터 정보들은 분류의 타겟 클래스 값과 유사할 수 있음\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('header','footer','quotes'),\n",
    "                               random_state=0)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('header','footer','quotes'),\n",
    "                               random_state=0)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "0     319\n",
      "1     389\n",
      "2     394\n",
      "3     392\n",
      "4     385\n",
      "5     395\n",
      "6     390\n",
      "7     396\n",
      "8     398\n",
      "9     397\n",
      "10    399\n",
      "11    396\n",
      "12    393\n",
      "13    396\n",
      "14    394\n",
      "15    398\n",
      "16    364\n",
      "17    376\n",
      "18    310\n",
      "19    251\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(news_data.target_names)\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 120756)\n",
      "(7532, 120756)\n"
     ]
    }
   ],
   "source": [
    "# 피터 벡터화 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "# 학습 데이터로 fit()된 Countervectorizer를 이용, 테스트 데이터 피처 벡터화 변환\n",
    "# (피처 개수가 동일해야 함)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "print(X_train_cnt_vect.shape)\n",
    "print(X_test_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7523898035050451\n"
     ]
    }
   ],
   "source": [
    "# 머신러닝 모델 학습/예측/평가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7841210833775889\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화 변환 : TF-IDF 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect,y_train)\n",
    "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774429102496017\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링 추가, ngram을 기본 (1,1)에서 (1,2)로 max_df=300으로 변경해 \n",
    "# 피처 벡터화 적용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2),max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[과제]\n",
    "Random Forest, SVM, DT, KNN을 적용 뉴스그룹에 대한 분류 예측을 수행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 33.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.7980616038236856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# 최적 C값 도출 튜닝 수행. cv는 3 Fold셋으로 설정\n",
    "params={'C':[5,10]}\n",
    "gcv_lr = GridSearchCV(lr_clf,param_grid=params,cv=3,\n",
    "                     scoring='accuracy',verbose=1)\n",
    "gcv_lr.fit(X_train_tfidf_vect,y_train)\n",
    "print(gcv_lr.best_params_)\n",
    "lr_pred = gcv_lr.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 소요\n",
    "# 사이킷런 파이프라인\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로 LogisticRegression 객체를\n",
    "# lr_clf 객체명으로 생성하는 pipeline 생성\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english',\n",
    "                                  ngram_range=(1,2),max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "pipeline.fit(X_train,y_train)\n",
    "pred=pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test,pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 소요\n",
    "# 사이킷런 파이프라인과 GridSearchCV와의 결합\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf',LogisticRegression())\n",
    "])\n",
    "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "         'tfidf_vect__max_df':[100,300,700],\n",
    "         'lr_clf__C':[1,5,10]}\n",
    "grid_cv_pipe = GridSearchCV(pipeline,param_grid=params,cv=3,\n",
    "                           scoring='accuracy',verbose=1)\n",
    "grid_cv_pipe.fit(X_train,y_train)\n",
    "print(grid_cv_pipe.best_params_,grid_cv_pipe.best_score_)\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 영화 평점 train 데이터 기반으로 다음 사항에 유의하여 감정분석 후 test 데이터를 이용해 최종 감정 예측을 수행하세요. \n",
    "* 네이버 영화 평점 데이터는 http://github.com/e9t/nsmc에서 ratings.txt(전체), ratings._train.txt(학습), rating_test.txt(테스트)를 다운로드 \n",
    "* 데이터 세트 요약 정보\n",
    "\n",
    "  - 칼럼 : id, document\n",
    "  - label : the sentiment class of the review (0:negative, 1: positive)\n",
    "  - ratings.txt : All 200K reviews\n",
    "  - ratings_test : 50K reviews held out for testing\n",
    "  - ratings_train : 150K reviews for training\n",
    "  \n",
    "* 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
    "* 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "* 한글 형태소 분석을 통해 형태소 단어로 토큰화(tokenizer 사용자 함수 작성 추천)\n",
    "  - 한글 형태소 엔진은 SNS 분석에 적합한 Okt 클래스를 이용\n",
    "  - morphs() 메서드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체 반환 \n",
    "  - Okt 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n",
    "* 사이킷런의 TfidVectorizor를 이용, TF-IDF 피처 모델을 생성(10분 이상 소요)\n",
    "* DT, RF, LR 을 이용하여 감성 분석 Classification 수행\n",
    "* 3개 분류 모델별 교차 검증 및 Parameter 최적화를 GridSearchCV 를 이용하여 수행(최적 분류 모델 선정)\n",
    "  - DT params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}\n",
    "  - RF params = {'n_estimators':[50,100, 200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}  \n",
    "  - LR params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\\\n",
    "    C는 규제 강도를 조절하는 alpha 값의 역수로 작을 수록 규제 강도가 큼\n",
    "  - param_grid=params , cv=3 ,scoring='accuracy', verbose=1(학습진행 상황 표시) \n",
    "* 학습데이터에 사용된 TfidVectorizer 객체 변수인 tfidf_vect를 이용해 transform()을 테스트 데이터의 document 칼럼에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7Q6L3WZAVMaxhuUOGy5Si",
   "collapsed_sections": [],
   "mount_file_id": "1L-Ec1RAhN4-y9XxhjoKTncR-VXm4rpo6",
   "name": "nlp10_Text Mining_guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
