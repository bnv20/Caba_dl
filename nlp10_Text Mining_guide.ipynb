{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05O_d01EF4aK"
   },
   "source": [
    "![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJFn9387KeE"
   },
   "source": [
    "## NLP, 텍스트 분석\n",
    "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\n",
    "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
    "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\n",
    "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
    "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
    " - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\n",
    " - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\n",
    " - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
    " - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
    " \n",
    "#### Text 분석 수행 프로세스\n",
    "- 텍스트 정규화\n",
    " - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
    "- 피처 벡터화 변환\n",
    " - Bag of Words : Count 기반, TF-IDF 기반\n",
    " - Word2Vec\n",
    "- ML 모델 수립 및 학습/예측/평가\n",
    "\n",
    "#### 텍스트 전처리 - 텍스트 정규화\n",
    "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
    "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
    "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
    " - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    " - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1614740772975,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "a8WINW-wETZH",
    "outputId": "85aa09e4-cc30-42c7-c5e9-1a1a5e45d272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1614740890871,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "H8IhYjPTFd-r",
    "outputId": "11a0e3a1-877b-40b4-93c3-f833e049c703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화(sent tokenize) : 마침표, 개행문자(\\n), 정규표현식\n",
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(sentences)\n",
    "print(type(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1614741158680,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "1gY7rmJvD5dW",
    "outputId": "9e5594a2-2102-4eb3-dff5-24ae268d74ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화(word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "print(type(words),len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1614741715154,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "aqQjF140E8KV",
    "outputId": "3f74a7ca-d7e4-4386-ea32-85e0b3eff83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해서 모든 단어를 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text) # 문장별 분리 토큰\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)\n",
    "print(type(word_tokens), len(word_tokens))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1614742024201,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zli4RcSnHdbD",
    "outputId": "75116589-3252-4935-c028-2d0a434436e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스톱워드 제거 : the, is, a, will와 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1614742208416,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "FAUm6nikIAIa",
    "outputId": "0e1b6432-a5d1-4064-d961-d0fea2ea73e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# NLTK english stopwords 갯수 확인\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1614742587642,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "rSE_VxExI2k1",
    "outputId": "44a97ecd-6b30-4a04-a6a2-49ffa7b37a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords 필터링을 통한 제거\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "  filtered_words = []\n",
    "  for word in sentence:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords:\n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "print(all_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1614743336098,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "o6D1fvVKKP-1",
    "outputId": "73a028ac-a49f-46d0-a7b1-eda228d84e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus aums amus\n",
      "fant fant\n"
     ]
    }
   ],
   "source": [
    "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
    "# Stemmer(LancasterStemmer)\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fancist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1614743257311,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zjLN9vwxMmBF",
    "outputId": "7dcf4623-5173-4477-cb3b-05c9a895c748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1614743401934,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "kePGySerLME9",
    "outputId": "a1f8a051-0286-4b0f-88ea-ee7c4b61c271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amuse aumses amuse\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtNVRIkBlTla"
   },
   "source": [
    "GPU vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9737,
     "status": "ok",
     "timestamp": 1614817435356,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zh0_dm0ukiiM",
    "outputId": "98516aa4-b78f-4004-dea6-29a4e39ee036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 7s 59ms/step - loss: 235.4992\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 242.3844\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 243.9395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model = Xception(weights = None,\n",
    "                 input_shape=(height,width,3),\n",
    "                 classes=num_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "x=np.random.random((num_samples,height,width,3))\n",
    "y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "model.fit(x,y,epochs=3,batch_size=16)\n",
    "model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1614817438600,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "4q2FMHd_ncQ0",
    "outputId": "126e18df-785d-4c06-a6b1-34dd0170fbad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:9초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40440,
     "status": "ok",
     "timestamp": 1614817585374,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "CSFC-eyjn2Fj",
    "outputId": "77649725-7830-4977-eca3-c8ab60964d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 17s 1s/step - loss: 233.7611\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 244.0772\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 242.1810\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  model = Xception(weights = None,\n",
    "                  input_shape=(height,width,3),\n",
    "                  classes=num_classes)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop')\n",
    "  x=np.random.random((num_samples,height,width,3))\n",
    "  y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "  model.fit(x,y,epochs=3,batch_size=16)\n",
    "  model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1614817590039,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "MPQI50EmoI1j",
    "outputId": "837eb25d-423a-4bc9-ea5a-aedd0599d4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:39초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 벡터화 : One-hot encoding\n",
    "Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.  \n",
    "단점 : 문맥 의미 반영 부족, 희소 행렬 문제.  \n",
    "BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.  \n",
    "피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화  \n",
    "카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여  \n",
    "TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등  \n",
    "파라미터  \n",
    "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외  \n",
    "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외  \n",
    "- max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "- stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "- n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "- analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
    "- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
    "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3,0,1],[0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO 형식\n",
    "from scipy import sparse\n",
    "data = np.array([3,1,2]) # 0이 아닌 데이터\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "sparse_coo = sparse.coo_matrix((data,(row_pos,col_pos)))\n",
    "print(sparse_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO vs CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "['A', 'B', 'C']\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 \n",
    "# BOW 인코딩한 수치 벡터로 전환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A':1,'B':2},{'B':3,'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "print(v.feature_names_)\n",
    "print(v.vocabulary_)\n",
    "print(v.transform({'C':4,'D':3}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print()\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \n",
    "# 보통 영어의 관사, 접속사, 한국어의 조사 등\n",
    "\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 20,\n",
       " 'is': 5,\n",
       " 'the': 13,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 21,\n",
       " 'is the': 6,\n",
       " 'the first': 14,\n",
       " 'first document': 4,\n",
       " 'second': 11,\n",
       " 'the second': 16,\n",
       " 'second document': 12,\n",
       " 'and': 0,\n",
       " 'third': 18,\n",
       " 'one': 10,\n",
       " 'and the': 1,\n",
       " 'the third': 17,\n",
       " 'third one': 19,\n",
       " 'is this': 7,\n",
       " 'this the': 22,\n",
       " 'last': 8,\n",
       " 'the last': 15,\n",
       " 'last document': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
    "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소  \n",
    "문서 d(document)와 단어 t 에 대해 다음과 같이 계산  \n",
    "\n",
    "tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
    "\n",
    "tf(d,t): term frequency. 특정한 단어의 빈도수  \n",
    "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수  \n",
    "n : 전체 문서의 수  \n",
    "df(t) : 단어  t 를 가진 문서의 수  \n",
    "idf(d,t)=log(n/(1+df(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"\"\"\n",
    "The Corpus of Contemporary American English (COCA) is the only large, \n",
    "genre-balanced corpus of American English. COCA is probably the most \n",
    "widely-used corpus of English, and it is related to many other corpora of \n",
    "English that we have created, which offer unparalleled insight into variation \n",
    "in English.\n",
    "\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'balanced', 'coca', 'contemporary', 'corpora', 'corpus', 'created', 'english', 'genre', 'insight', 'large', 'offer', 'probably', 'related', 'unparalleled', 'used', 'variation', 'widely']\n",
      "\n",
      "{'corpus': 5, 'contemporary': 3, 'american': 0, 'english': 7, 'coca': 2, 'large': 10, 'genre': 8, 'balanced': 1, 'probably': 12, 'widely': 17, 'used': 15, 'related': 13, 'corpora': 4, 'created': 6, 'offer': 11, 'unparalleled': 14, 'insight': 9, 'variation': 16}\n",
      "\n",
      "[[0.26726124 0.13363062 0.26726124 0.13363062 0.13363062 0.40089186\n",
      "  0.13363062 0.6681531  0.13363062 0.13363062 0.13363062 0.13363062\n",
      "  0.13363062 0.13363062 0.13363062 0.13363062 0.13363062 0.13363062]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "print(tfidv.get_feature_names())\n",
    "print()\n",
    "print(tfidv.vocabulary_)\n",
    "print()\n",
    "print(tfidv.transform(corpus).toarray())\n",
    "print(tfidv.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy 설치\n",
    "# Java 환경 세팅\n",
    "# JPype1-1.1.2-cp37-cp37m-win_amd64.whl 다운로드 후 C:\\Users\\admin 경로에 저장\n",
    "# (caba)C:\\Users\\admin>pip install JPype1-1.1.2-cp37-cp37m-win_amd64.whl\n",
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KoNLPy] https://mr-doosun.tistory.com/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석으로 문장을 단어로 분할\n",
    "okt = Okt()\n",
    "print(okt.morphs('단독입찰보다 복수입찰의 경우'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['항공기', '체계', '종합', '개발', '경험']\n",
      "['나', '프로젝트', '흥미진진']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
    "print(okt.nouns('나는 프로젝트를 하고있는데 흥미진진하고 재미있다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "# 원형\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True, join=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n",
      "['꽃', '파란', '하늘']\n"
     ]
    }
   ],
   "source": [
    "# 명사인 품사만 선별해 리스트에 담기\n",
    "morph =  okt.pos('아름다운 꽃과 파란 하늘')\n",
    "print(morph)\n",
    "noun_list = []\n",
    "for word, tag in morph:\n",
    "    if tag == 'Noun':\n",
    "        noun_list.append(word)\n",
    "print(noun_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 아래와 같이 분할하세요.  \n",
    "\n",
    "- 명사 '나는 오늘 방콕에 가고싶다.'\n",
    "- 원형 '나는 오늘 방콕에 갔다.'\n",
    "- 형태소 '친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.'\n",
    "- 형태소/태그' 나는 오늘도 장에 가고싶다.'\n",
    "- 정규화, 원형' 나는 오늘 장에 가고싶을깤ㅋㅋ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사\n",
      "['나', '오늘', '방콕']\n",
      "원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('방콕', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('.', 'Punctuation')]\n",
      "형태소\n",
      "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
      "형태소/태그\n",
      "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가다/Verb', './Punctuation']\n",
      "정규화, 원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "malist1 = okt.nouns('나는 오늘 방콕에 가고싶다.')\n",
    "malist2 = okt.pos('나는 오늘 방콕에 갔다.', norm=True, stem=True)\n",
    "malist3 = okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.')\n",
    "malist4 = okt.pos('나는 오늘도 장에 가고싶다.', norm=True, stem=True, join=True)\n",
    "malist5 = okt.pos('나는 오늘 장에 가고싶을깤ㅋㅋ?', norm=True, stem=True)\n",
    "print('명사')\n",
    "print(malist1)\n",
    "print('원형')\n",
    "print(malist2)\n",
    "print('형태소')\n",
    "print(malist3)\n",
    "print('형태소/태그')\n",
    "print(malist4)\n",
    "print('정규화, 원형')\n",
    "print(malist5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 분류\n",
    "- 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측\n",
    "- 텍스트를 피처 벡터화로 변환, 희소 행렬로 만들고 로지스틱 회귀를 이용해 분류 수행\n",
    "- Count 기반 과 TF-IDF 기반의 벡터화를 각각 적용, 성능 비교\n",
    "- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼파라미터 튜닝을 일괄적으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=0)\n",
    "news_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  1 15 ...  0  5  8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(news_data.target)\n",
    "a = pd.Series(news_data.target).unique()\n",
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _20newsgroups_dataset:\\n\\nThe 20 newsgroups text dataset\\n------------------------------\\n\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\n\\nThis module contains two loaders. The first one,\\n:func:`sklearn.datasets.fetch_20newsgroups`,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\\nwith custom parameters so as to extract feature vectors.\\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\n\\n**Data Set Characteristics:**\\n\\n    =================   ==========\\n    Classes                     20\\n    Samples total            18846\\n    Dimensionality               1\\n    Features                  text\\n    =================   ==========\\n\\nUsage\\n~~~~~\\n\\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original `20 newsgroups website`_, extracts the archive contents\\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\\n:func:`sklearn.datasets.load_files` on either the training or\\ntesting set folder, or both of them::\\n\\n  >>> from sklearn.datasets import fetch_20newsgroups\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\n\\n  >>> from pprint import pprint\\n  >>> pprint(list(newsgroups_train.target_names))\\n  [\\'alt.atheism\\',\\n   \\'comp.graphics\\',\\n   \\'comp.os.ms-windows.misc\\',\\n   \\'comp.sys.ibm.pc.hardware\\',\\n   \\'comp.sys.mac.hardware\\',\\n   \\'comp.windows.x\\',\\n   \\'misc.forsale\\',\\n   \\'rec.autos\\',\\n   \\'rec.motorcycles\\',\\n   \\'rec.sport.baseball\\',\\n   \\'rec.sport.hockey\\',\\n   \\'sci.crypt\\',\\n   \\'sci.electronics\\',\\n   \\'sci.med\\',\\n   \\'sci.space\\',\\n   \\'soc.religion.christian\\',\\n   \\'talk.politics.guns\\',\\n   \\'talk.politics.mideast\\',\\n   \\'talk.politics.misc\\',\\n   \\'talk.religion.misc\\']\\n\\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\\nattribute is the integer index of the category::\\n\\n  >>> newsgroups_train.filenames.shape\\n  (11314,)\\n  >>> newsgroups_train.target.shape\\n  (11314,)\\n  >>> newsgroups_train.target[:10]\\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\n\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\n:func:`sklearn.datasets.fetch_20newsgroups` function::\\n\\n  >>> cats = [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\n\\n  >>> list(newsgroups_train.target_names)\\n  [\\'alt.atheism\\', \\'sci.space\\']\\n  >>> newsgroups_train.filenames.shape\\n  (1073,)\\n  >>> newsgroups_train.target.shape\\n  (1073,)\\n  >>> newsgroups_train.target[:10]\\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\n\\nConverting text to vectors\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\n``sklearn.feature_extraction.text`` as demonstrated in the following\\nexample that extract `TF-IDF`_ vectors of unigram tokens\\nfrom a subset of 20news::\\n\\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\\n  >>> categories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n  ...               \\'comp.graphics\\', \\'sci.space\\']\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       categories=categories)\\n  >>> vectorizer = TfidfVectorizer()\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> vectors.shape\\n  (2034, 34118)\\n\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features)::\\n\\n  >>> vectors.nnz / float(vectors.shape[0])\\n  159.01327...\\n\\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \\nreturns ready-to-use token counts features instead of file names.\\n\\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\\n\\n\\nFiltering text for more realistic training\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren\\'t from this window of time.\\n\\nFor example, let\\'s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score::\\n\\n  >>> from sklearn.naive_bayes import MultinomialNB\\n  >>> from sklearn import metrics\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.88213...\\n\\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what\\'s going on inside this classifier?)\\n\\nLet\\'s take a look at what the most informative features are:\\n\\n  >>> import numpy as np\\n  >>> def show_top10(classifier, vectorizer, categories):\\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\\n  ...     for i, category in enumerate(categories):\\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n  ...\\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\\n  alt.atheism: edu it and in you that is of to the\\n  comp.graphics: edu in graphics it is for and of to the\\n  sci.space: edu it that is in and space to of the\\n  talk.religion.misc: not it you in is that and to of the\\n\\n\\nYou can now see many things that these features have overfit to:\\n\\n- Almost every group is distinguished by whether headers such as\\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\\n- Another significant feature involves whether the sender is affiliated with\\n  a university, as indicated either by their headers or their signature.\\n- The word \"article\" is a significant feature, based on how often people quote\\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\\n  wrote:\"\\n- Other features match the names and e-mail addresses of particular people who\\n  were posting at the time.\\n\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\n\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called **remove**, telling it what kinds of information to strip out\\nof each file. **remove** should be a tuple containing any subset of\\n``(\\'headers\\', \\'footers\\', \\'quotes\\')``, telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\n\\n  >>> newsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n  ...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                      categories=categories)\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')\\n  0.77310...\\n\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\n\\n  >>> newsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n  ...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n  ...                                       categories=categories)\\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\\n  >>> clf = MultinomialNB(alpha=.01)\\n  >>> clf.fit(vectors, newsgroups_train.target)\\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\n\\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\\n  >>> pred = clf.predict(vectors_test)\\n  >>> metrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n  0.76995...\\n\\nSome other classifiers cope better with this harder version of the task. Try\\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\\nthe ``--filter`` option to compare the results.\\n\\n.. topic:: Recommendation\\n\\n  When evaluating text classifiers on the 20 Newsgroups data, you\\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\\n  setting ``remove=(\\'headers\\', \\'footers\\', \\'quotes\\')``. The F-score will be\\n  lower because it is more realistic.\\n\\n.. topic:: Examples\\n\\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\\n\\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 7532\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정규화\n",
    "# 뉴스그룹 기사내용을 제외하고 다른 정보 제거\n",
    "# 제목, 소속, 이메일 등 헤더와 푸터 정보들은 분류의 타겟 클래스 값과 유사할 수 있음\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('header','footer','quotes'),\n",
    "                               random_state=0)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('header','footer','quotes'),\n",
    "                               random_state=0)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "0     319\n",
      "1     389\n",
      "2     394\n",
      "3     392\n",
      "4     385\n",
      "5     395\n",
      "6     390\n",
      "7     396\n",
      "8     398\n",
      "9     397\n",
      "10    399\n",
      "11    396\n",
      "12    393\n",
      "13    396\n",
      "14    394\n",
      "15    398\n",
      "16    364\n",
      "17    376\n",
      "18    310\n",
      "19    251\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(news_data.target_names)\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 120756)\n",
      "(7532, 120756)\n"
     ]
    }
   ],
   "source": [
    "# 피터 벡터화 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "# 학습 데이터로 fit()된 Countervectorizer를 이용, 테스트 데이터 피처 벡터화 변환\n",
    "# (피처 개수가 동일해야 함)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "print(X_train_cnt_vect.shape)\n",
    "print(X_test_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7523898035050451\n"
     ]
    }
   ],
   "source": [
    "# 머신러닝 모델 학습/예측/평가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7841210833775889\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화 변환 : TF-IDF 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect,y_train)\n",
    "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774429102496017\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링 추가, ngram을 기본 (1,1)에서 (1,2)로 max_df=300으로 변경해 \n",
    "# 피처 벡터화 적용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2),max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[과제]\n",
    "Random Forest, SVM, DT, KNN을 적용 뉴스그룹에 대한 분류 예측을 수행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 33.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.7980616038236856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# 최적 C값 도출 튜닝 수행. cv는 3 Fold셋으로 설정\n",
    "params={'C':[5,10]}\n",
    "gcv_lr = GridSearchCV(lr_clf,param_grid=params,cv=3,\n",
    "                     scoring='accuracy',verbose=1)\n",
    "gcv_lr.fit(X_train_tfidf_vect,y_train)\n",
    "print(gcv_lr.best_params_)\n",
    "lr_pred = gcv_lr.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 소요\n",
    "# 사이킷런 파이프라인\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로 LogisticRegression 객체를\n",
    "# lr_clf 객체명으로 생성하는 pipeline 생성\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english',\n",
    "                                  ngram_range=(1,2),max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "pipeline.fit(X_train,y_train)\n",
    "pred=pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test,pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 소요\n",
    "# 사이킷런 파이프라인과 GridSearchCV와의 결합\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf',LogisticRegression())\n",
    "])\n",
    "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "         'tfidf_vect__max_df':[100,300,700],\n",
    "         'lr_clf__C':[1,5,10]}\n",
    "grid_cv_pipe = GridSearchCV(pipeline,param_grid=params,cv=3,\n",
    "                           scoring='accuracy',verbose=1)\n",
    "grid_cv_pipe.fit(X_train,y_train)\n",
    "print(grid_cv_pipe.best_params_,grid_cv_pipe.best_score_)\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://wikidocs.net/4308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네이버 영화 평점 train 데이터 기반으로 다음 사항에 유의하여 감정분석 후 test 데이터를 이용해 최종 감정 예측을 수행하세요. \n",
    "* 네이버 영화 평점 데이터는 http://github.com/e9t/nsmc에서 ratings.txt(전체), ratings._train.txt(학습), rating_test.txt(테스트)를 다운로드 \n",
    "* 데이터 세트 요약 정보\n",
    "\n",
    "  - 칼럼 : id, document\n",
    "  - label : the sentiment class of the review (0:negative, 1: positive)\n",
    "  - ratings.txt : All 200K reviews\n",
    "  - ratings_test : 50K reviews held out for testing\n",
    "  - ratings_train : 150K reviews for training\n",
    "  \n",
    "* 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
    "* 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "* 한글 형태소 분석을 통해 형태소 단어로 토큰화(tokenizer 사용자 함수 작성 추천)\n",
    "  - 한글 형태소 엔진은 SNS 분석에 적합한 Okt 클래스를 이용\n",
    "  - morphs() 메서드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체 반환 \n",
    "  - Okt 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n",
    "* 사이킷런의 TfidVectorizor를 이용, TF-IDF 피처 모델을 생성(10분 이상 소요)\n",
    "* DT, RF, LR 을 이용하여 감성 분석 Classification 수행\n",
    "* 3개 분류 모델별 교차 검증 및 Parameter 최적화를 GridSearchCV 를 이용하여 수행(최적 분류 모델 선정)\n",
    "  - DT params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}\n",
    "  - RF params = {'n_estimators':[50,100, 200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}  \n",
    "  - LR params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\\\n",
    "    C는 규제 강도를 조절하는 alpha 값의 역수로 작을 수록 규제 강도가 큼\n",
    "  - param_grid=params , cv=3 ,scoring='accuracy', verbose=1(학습진행 상황 표시) \n",
    "* 학습데이터에 사용된 TfidfVectorizer 객체 변수인 tfidf_vect를 이용해 transform()을 테스트 데이터의 document 칼럼에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글 텍스트 처리\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('./dataset/ratings_train.txt',sep='\\t')\n",
    "test_df = pd.read_csv('./dataset/ratings_test.txt', sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_df=train_df.fillna(' ')\n",
    "train_df['document'] = train_df['document'].apply(lambda x : re.sub(r\"\\d+\",\" \",x))\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df.document = test_df.document.apply(lambda x : re.sub(r\"\\d+\", \" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\caba\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "\n",
    "okt = Okt()\n",
    "def okt_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=okt_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(train_df.document)\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df.document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.5} 0.8591\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression 을 이용하여 감성 분석 Classification 수행.\n",
    "# 교차검증(cv=3) 및 하이퍼파라미터 튜닝\n",
    "# LR params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train, train_df.label)\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트를 이용해 최종 감성 분석 예측 수행\n",
    "# 학습 데이터를 적용한 TfidfVectorizer 를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함.\n",
    "# 이유는 학습 시 설정된 TfidfVectorizer의 피처 개수와 테스트 데이터를 변환할 피처 개수가 같아야 \n",
    "# 하기 때문임 \n",
    "# 학습 데이터에 사용된 TfidfVectorizer 객체변수인 tfidf_vect를 이용해 transform()을 테스트 \n",
    "# 데이터의 document 칼럼에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df.document)\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "print(accuracy_score(test_df.label,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf을 이용하여 감성 분석 Classification 수행\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "params = {'n_estimators':[50,100, 200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv=3, scoring= 'accuracy',verbose=0)\n",
    "grid_cv.fit(tfidf_matrix_train,train_df.label)\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_,4))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df.document)\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "rf_preds = best_estimator.predict(tfidf_matrix_test)\n",
    "print('rf 정확도:', accuracy_score(test_df.label,rf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt를 이용하여 감성 분석 Classification 수행\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],\\\n",
    "              'min_samples_leaf':[1,5,8]}\n",
    "gcv_dt = GridSearchCV(dt_clf,param_grid = params, cv=3, scoring='accuracy',verbose=0)\n",
    "gcv_dt.fit(tfidf_matrix_train,train_df.label)\n",
    "print(gcv_dt.best_params_, round(gcv_dt.best_score_,4))\n",
    "\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df.document)\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "dt_preds = best_estimator.predict(tfidf_matrix_test)\n",
    "print('dt 정확도:', accuracy_score(test_df.label,dt_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 모델링\n",
    "- 머신러닝 기반의 토픽 모델링을 적용해 문서 집합에 숨어 있는 주제를 찾아냄\n",
    "- 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것에 반해 머신러닝 기반의 토픽 모델링은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출\n",
    "- LSA(Latent Sementic Analysis) 와 LDA(Latent Dirichlet Allocation) 기법\n",
    " - LSA는 단어-문서행렬(Word-Document Matrix), 단어-문맥행렬(window based co-occurrence matrix) 등 입력 데이터에 특이값 분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우면서 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론\n",
    " - LDA는 미리 알고 있는 주제별 단어수 분포를 바탕으로, 주어진 문서에서 발견된 단어수 분포를 분석, 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape: (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cats = ['rec.motorcycles','rec.sport.baseball','comp.graphics','comp.windows.x',\n",
    "        'talk.politics.mideast','soc.religion.christian','sci.electronics','sci.med']\n",
    "\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            categories = cats, random_state=0)\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english',\n",
    "                            ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape:', feat_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=8, random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
      "Topic # 1\n",
      "don just like know people said think time ve didn right going say ll way\n",
      "Topic # 2\n",
      "image file jpeg program gif images output format files color entry 00 use bit 03\n",
      "Topic # 3\n",
      "like know don think use does just good time book read information people used post\n",
      "Topic # 4\n",
      "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
      "Topic # 5\n",
      "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
      "Topic # 6\n",
      "god people jesus church believe christ does christian say think christians bible faith sin life\n",
      "Topic # 7\n",
      "use dos thanks windows using window does display help like problem server need know run\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes = topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "display_topics(lda, feature_names, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 군집화\n",
    "- 비슷한 텍스트 구성의 문서를 군집화하여 같은 카테고리 소속으로 분류\n",
    "- 학습 데이터 세트가 필요없는 비지도학습 기반으로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  \n",
       "0                                                ...  \n",
       "1                                                ...  \n",
       "2                                                ...  \n",
       "3                                                ...  \n",
       "4                                                ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glob 모듈의 glob 함수는 path로 지정한 디렉토리 밑에 있는 모든 .data 파일들의 파일명을 \n",
    "# 리스트로 반환\n",
    "\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "path = 'dataset/topics'\n",
    "all_files = glob.glob(os.path.join(path,'*.data'))\n",
    "\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    \n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    \n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "    \n",
    "document_df = pd.DataFrame({'filename':filename_list,'opinion_text':opinion_text})\n",
    "document_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The translate method will convert all punctuations defined in the keys of remove_punct_dict \\nto their respective values which are None. For example, \"hi!!..\".translate(remove_punct_dict) \\nwill return Out: \"hi\"'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "# out: {33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39:\n",
    "lemma = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemma.lemmatize(token) for token in tokens]\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "'''The translate method will convert all punctuations defined in the keys of remove_punct_dict \n",
    "to their respective values which are None. For example, \"hi!!..\".translate(remove_punct_dict) \n",
    "will return Out: \"hi\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\n",
    "                            ngram_range=(1,2), min_df=0.05,max_df=0.85)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  cluster_label  \n",
       "0                                                ...              0  \n",
       "1                                                ...              1  \n",
       "2                                                ...              3  \n",
       "3                                                ...              3  \n",
       "4                                                ...              3  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>performance_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sound_ipod_nano_8gb</td>\n",
       "      <td>headphone jack i got a clear case for it a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>video_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "2    battery-life_amazon_kindle   \n",
       "3    battery-life_ipod_nano_8gb   \n",
       "4   battery-life_netbook_1005ha   \n",
       "26   performance_netbook_1005ha   \n",
       "42          sound_ipod_nano_8gb   \n",
       "49          video_ipod_nano_8gb   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "2                                                 ...              3  \n",
       "3                                                 ...              3  \n",
       "4                                                 ...              3  \n",
       "26                                                ...              3  \n",
       "42      headphone jack i got a clear case for it a...              3  \n",
       "49                                                ...              3  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 적합한 군집개수를 확인하세요.\n",
    "\n",
    "# document_df[document_df.cluster_label==0].sort_values(by='filename') # 차량용 네비\n",
    "# document_df[document_df.cluster_label==1].sort_values(by='filename') # 호텔\n",
    "# document_df[document_df.cluster_label==2].sort_values(by='filename') # 자동차\n",
    "document_df[document_df.cluster_label==3].sort_values(by='filename') # 차량용 네비\n",
    "# document_df[document_df.cluster_label==4].sort_values(by='filename') # 포터블 전자기긱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.sort_values(by='cluster_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4611)\n",
      "[[0.01005322 0.         0.         ... 0.00706287 0.         0.        ]\n",
      " [0.         0.00099499 0.00174637 ... 0.         0.00183397 0.00144581]\n",
      " [0.         0.00092551 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 군집별 핵심 단어 추출\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "print(cluster_centers.shape)\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Cluster 0\n",
      "Top features: ['screen', 'battery', 'keyboard', 'battery life', 'life', 'kindle', 'direction', 'video', 'size', 'voice']\n",
      "Review 파일명: ['accuracy_garmin_nuvi_255W_gps', 'battery-life_amazon_kindle', 'battery-life_ipod_nano_8gb', 'battery-life_netbook_1005ha', 'buttons_amazon_kindle', 'directions_garmin_nuvi_255W_gps', 'display_garmin_nuvi_255W_gps']\n",
      "### Cluster 1\n",
      "Top features: ['room', 'hotel', 'service', 'staff', 'food', 'location', 'bathroom', 'clean', 'price', 'parking']\n",
      "Review 파일명: ['bathroom_bestwestern_hotel_sfo', 'food_holiday_inn_london', 'food_swissotel_chicago', 'free_bestwestern_hotel_sfo', 'location_bestwestern_hotel_sfo', 'location_holiday_inn_london', 'parking_bestwestern_hotel_sfo']\n",
      "### Cluster 2\n",
      "Top features: ['interior', 'seat', 'mileage', 'comfortable', 'gas', 'gas mileage', 'transmission', 'car', 'performance', 'quality']\n",
      "Review 파일명: ['comfort_honda_accord_2008', 'comfort_toyota_camry_2007', 'gas_mileage_toyota_camry_2007', 'interior_honda_accord_2008', 'interior_toyota_camry_2007', 'mileage_honda_accord_2008', 'performance_honda_accord_2008']\n"
     ]
    }
   ],
   "source": [
    "# 군집별 top n 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명들을 반환\n",
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    \n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "    \n",
    "    for cluster_num in range(clusters_num):\n",
    "        # 개별 군집병 정보를 담을 데이터 초기화\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        \n",
    "        # top n 피처 단어를 구함\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num,:top_n_features]\n",
    "        top_features = [feature_names[ind] for ind in top_feature_indexes]\n",
    "        \n",
    "        # top_feature_indexes를 이용해 해당 피처 단어와 중심 위치 상대값 구함\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num,top_feature_indexes].tolist()\n",
    "        \n",
    "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값,파일명 입력\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_label']==cluster_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        cluster_details[cluster_num]['filenames']=filenames\n",
    "        \n",
    "    return cluster_details\n",
    "\n",
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('### Cluster {0}'.format(cluster_num))\n",
    "        print('Top features:',cluster_detail['top_features'])\n",
    "        print('Review 파일명:',cluster_detail['filenames'][:7] )\n",
    "        \n",
    "feature_names = tfidf_vect.get_feature_names()\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster,\n",
    "                                      cluster_data=document_df, \n",
    "                                      feature_names=feature_names,\n",
    "                                     clusters_num=3, top_n_features=10)\n",
    "print_cluster_details(cluster_details)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7Q6L3WZAVMaxhuUOGy5Si",
   "collapsed_sections": [],
   "mount_file_id": "1L-Ec1RAhN4-y9XxhjoKTncR-VXm4rpo6",
   "name": "nlp10_Text Mining_guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
