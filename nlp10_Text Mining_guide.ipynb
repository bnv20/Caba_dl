{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05O_d01EF4aK"
   },
   "source": [
    "![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZJFn9387KeE"
   },
   "source": [
    "## NLP, 텍스트 분석\n",
    "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\n",
    "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
    "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\n",
    "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
    "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
    " - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\n",
    " - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\n",
    " - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
    " - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
    " \n",
    "#### Text 분석 수행 프로세스\n",
    "- 텍스트 정규화\n",
    " - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
    "- 피처 벡터화 변환\n",
    " - Bag of Words : Count 기반, TF-IDF 기반\n",
    " - Word2Vec\n",
    "- ML 모델 수립 및 학습/예측/평가\n",
    "\n",
    "#### 텍스트 전처리 - 텍스트 정규화\n",
    "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
    "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
    "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
    " - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
    " - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1614740772975,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "a8WINW-wETZH",
    "outputId": "85aa09e4-cc30-42c7-c5e9-1a1a5e45d272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1614740890871,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "H8IhYjPTFd-r",
    "outputId": "11a0e3a1-877b-40b4-93c3-f833e049c703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화(sent tokenize) : 마침표, 개행문자(\\n), 정규표현식\n",
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(sentences)\n",
    "print(type(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1614741158680,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "1gY7rmJvD5dW",
    "outputId": "9e5594a2-2102-4eb3-dff5-24ae268d74ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화(word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "print(type(words),len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1614741715154,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "aqQjF140E8KV",
    "outputId": "3f74a7ca-d7e4-4386-ea32-85e0b3eff83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해서 모든 단어를 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text) # 문장별 분리 토큰\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)\n",
    "print(type(word_tokens), len(word_tokens))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1614742024201,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zli4RcSnHdbD",
    "outputId": "75116589-3252-4935-c028-2d0a434436e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스톱워드 제거 : the, is, a, will와 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1614742208416,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "FAUm6nikIAIa",
    "outputId": "0e1b6432-a5d1-4064-d961-d0fea2ea73e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# NLTK english stopwords 갯수 확인\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1614742587642,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "rSE_VxExI2k1",
    "outputId": "44a97ecd-6b30-4a04-a6a2-49ffa7b37a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords 필터링을 통한 제거\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "  filtered_words = []\n",
    "  for word in sentence:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords:\n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "print(all_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1614743336098,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "o6D1fvVKKP-1",
    "outputId": "73a028ac-a49f-46d0-a7b1-eda228d84e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus aums amus\n",
      "fant fant\n"
     ]
    }
   ],
   "source": [
    "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
    "# Stemmer(LancasterStemmer)\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fancist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1614743257311,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zjLN9vwxMmBF",
    "outputId": "7dcf4623-5173-4477-cb3b-05c9a895c748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1614743401934,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "kePGySerLME9",
    "outputId": "a1f8a051-0286-4b0f-88ea-ee7c4b61c271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amuse aumses amuse\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtNVRIkBlTla"
   },
   "source": [
    "GPU vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9737,
     "status": "ok",
     "timestamp": 1614817435356,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "zh0_dm0ukiiM",
    "outputId": "98516aa4-b78f-4004-dea6-29a4e39ee036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 7s 59ms/step - loss: 235.4992\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 242.3844\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 243.9395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model = Xception(weights = None,\n",
    "                 input_shape=(height,width,3),\n",
    "                 classes=num_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "x=np.random.random((num_samples,height,width,3))\n",
    "y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "model.fit(x,y,epochs=3,batch_size=16)\n",
    "model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1614817438600,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "4q2FMHd_ncQ0",
    "outputId": "126e18df-785d-4c06-a6b1-34dd0170fbad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:9초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40440,
     "status": "ok",
     "timestamp": 1614817585374,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "CSFC-eyjn2Fj",
    "outputId": "77649725-7830-4977-eca3-c8ab60964d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 17s 1s/step - loss: 233.7611\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 244.0772\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 11s 1s/step - loss: 242.1810\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  model = Xception(weights = None,\n",
    "                  input_shape=(height,width,3),\n",
    "                  classes=num_classes)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='rmsprop')\n",
    "  x=np.random.random((num_samples,height,width,3))\n",
    "  y=np.random.random((num_samples,num_classes))\n",
    "\n",
    "  model.fit(x,y,epochs=3,batch_size=16)\n",
    "  model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1614817590039,
     "user": {
      "displayName": "kevin park",
      "photoUrl": "",
      "userId": "02703084888761299921"
     },
     "user_tz": -540
    },
    "id": "MPQI50EmoI1j",
    "outputId": "837eb25d-423a-4bc9-ea5a-aedd0599d4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린시간:39초\n"
     ]
    }
   ],
   "source": [
    "print('걸린시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 벡터화 : One-hot encoding\n",
    "Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.  \n",
    "단점 : 문맥 의미 반영 부족, 희소 행렬 문제.  \n",
    "BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.  \n",
    "피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화  \n",
    "카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여  \n",
    "TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등  \n",
    "파라미터  \n",
    "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외  \n",
    "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외  \n",
    "- max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "- stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "- n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "- analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
    "- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
    "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3,0,1],[0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO 형식\n",
    "from scipy import sparse\n",
    "data = np.array([3,1,2]) # 0이 아닌 데이터\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "sparse_coo = sparse.coo_matrix((data,(row_pos,col_pos)))\n",
    "print(sparse_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO vs CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "['A', 'B', 'C']\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 \n",
    "# BOW 인코딩한 수치 벡터로 전환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A':1,'B':2},{'B':3,'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "print(v.feature_names_)\n",
    "print(v.vocabulary_)\n",
    "print(v.transform({'C':4,'D':3}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print()\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \n",
    "# 보통 영어의 관사, 접속사, 한국어의 조사 등\n",
    "\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 20,\n",
       " 'is': 5,\n",
       " 'the': 13,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 21,\n",
       " 'is the': 6,\n",
       " 'the first': 14,\n",
       " 'first document': 4,\n",
       " 'second': 11,\n",
       " 'the second': 16,\n",
       " 'second document': 12,\n",
       " 'and': 0,\n",
       " 'third': 18,\n",
       " 'one': 10,\n",
       " 'and the': 1,\n",
       " 'the third': 17,\n",
       " 'third one': 19,\n",
       " 'is this': 7,\n",
       " 'this the': 22,\n",
       " 'last': 8,\n",
       " 'the last': 15,\n",
       " 'last document': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
    "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소  \n",
    "문서 d(document)와 단어 t 에 대해 다음과 같이 계산  \n",
    "\n",
    "tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
    "\n",
    "tf(d,t): term frequency. 특정한 단어의 빈도수  \n",
    "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수  \n",
    "n : 전체 문서의 수  \n",
    "df(t) : 단어  t 를 가진 문서의 수  \n",
    "idf(d,t)=log(n/(1+df(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"\"\"\n",
    "The Corpus of Contemporary American English (COCA) is the only large, \n",
    "genre-balanced corpus of American English. COCA is probably the most \n",
    "widely-used corpus of English, and it is related to many other corpora of \n",
    "English that we have created, which offer unparalleled insight into variation \n",
    "in English.\n",
    "\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'balanced', 'coca', 'contemporary', 'corpora', 'corpus', 'created', 'english', 'genre', 'insight', 'large', 'offer', 'probably', 'related', 'unparalleled', 'used', 'variation', 'widely']\n",
      "\n",
      "{'corpus': 5, 'contemporary': 3, 'american': 0, 'english': 7, 'coca': 2, 'large': 10, 'genre': 8, 'balanced': 1, 'probably': 12, 'widely': 17, 'used': 15, 'related': 13, 'corpora': 4, 'created': 6, 'offer': 11, 'unparalleled': 14, 'insight': 9, 'variation': 16}\n",
      "\n",
      "[[0.26726124 0.13363062 0.26726124 0.13363062 0.13363062 0.40089186\n",
      "  0.13363062 0.6681531  0.13363062 0.13363062 0.13363062 0.13363062\n",
      "  0.13363062 0.13363062 0.13363062 0.13363062 0.13363062 0.13363062]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "print(tfidv.get_feature_names())\n",
    "print()\n",
    "print(tfidv.vocabulary_)\n",
    "print()\n",
    "print(tfidv.transform(corpus).toarray())\n",
    "print(tfidv.transform(['This is the second document']).toarray())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7Q6L3WZAVMaxhuUOGy5Si",
   "collapsed_sections": [],
   "mount_file_id": "1L-Ec1RAhN4-y9XxhjoKTncR-VXm4rpo6",
   "name": "nlp10_Text Mining_guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
