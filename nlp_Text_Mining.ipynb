{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "nlp_Text Mining.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnv20/Caba_nlp/blob/main/nlp_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i42Vu0DkxFtV"
      },
      "source": [
        "![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTIHPoXCuDYV"
      },
      "source": [
        "## NLP, 텍스트 분석\n",
        "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\n",
        "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
        "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
        " - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\n",
        " - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\n",
        " - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
        " - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
        " \n",
        "#### Text 분석 수행 프로세스\n",
        "- 텍스트 정규화\n",
        " - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
        "- 피처 벡터화 변환\n",
        " - Bag of Words : Count 기반, TF-IDF 기반\n",
        " - Word2Vec\n",
        "- ML 모델 수립 및 학습/예측/평가\n",
        "\n",
        "#### 텍스트 전처리 - 텍스트 정규화\n",
        "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
        "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
        "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
        "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
        " - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
        " - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrpRBkN8uPev",
        "outputId": "f50e16e4-564f-4522-b7fc-4ec45f75133d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzoDBR0BuDYZ"
      },
      "source": [
        "# conda install nltk (토큰화를 위한 API 제공)\n",
        "# Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, \n",
        "# by using an unsupervised algorithm to build a model for abbreviation words, \n",
        "# collocations, and words that start sentences. \n",
        "# It must be trained on a large collection of plaintext in the target language before it can be used."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6L11PU8uDYa",
        "outputId": "1d7397ef-38e7-423c-ae24-cac92f0ab5c5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKQklMBWuDYb",
        "outputId": "85cb9a47-ba2f-40da-a02d-8ed2648cdb4b"
      },
      "source": [
        "# 문장 토큰화(sent tokenize) : 마침표, 개행문자(\\n), 정규표현식\n",
        "from nltk import sent_tokenize\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(sentences)\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdV7oVqKuDYb",
        "outputId": "14576e72-bc41-4632-c92d-fe7e66180831"
      },
      "source": [
        "# 단어 토큰화(word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
        "from nltk import word_tokenize \n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n",
        "print(type(words),len(words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
            "<class 'list'> 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDF0xYSduDYc",
        "outputId": "06a5bef4-fa42-4530-9681-14260e13dd07"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text) # 문장별 분리 토큰\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
        "    return word_tokens\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens), len(word_tokens))   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkKcp7nfuDYc",
        "outputId": "2164ef4a-4e83-4a8e-d5cb-cee0c0dcc40d"
      },
      "source": [
        "# 스톱 워드 제거 : is, the, a, will 와 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJvD-nhOuDYc",
        "outputId": "c557b9b5-482b-4d2e-e444-caa7f9806866"
      },
      "source": [
        "# NLTK의 english stopwords 갯수 확인\n",
        "print('영어 stop words 갯수:', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY1Unu_5uDYd",
        "outputId": "0bb40d3e-795f-46cb-e7b9-5210629ec93d"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\n",
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens =[]\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "print(all_tokens)       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brm-uZn7uDYd",
        "outputId": "f774e230-f525-48ca-e901-3a07459caa52"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법 : Stemming, Lemmatization\n",
        "# Stemmer(LancasterStemmer)\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiMoAUHuDYd",
        "outputId": "6eb1affd-c008-447b-e464-37108d5aef5a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rKtFhhTuDYd",
        "outputId": "17826fcc-f6c8-46b6-bc5a-12a2026afb8f"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing', 'v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_MV-YX5uDYe"
      },
      "source": [
        "#### 피처 벡터화 : One-hot encoding\n",
        "- Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델\n",
        "- 단점 : 문맥 의미 반영 부족, 희소 행렬 문제\n",
        "- BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
        "- 피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화\n",
        "- 카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여\n",
        "- TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등\n",
        "- 파라미터\n",
        "  - max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
        "  - min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외\n",
        "  - max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
        "  - stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
        "  - n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
        "  - analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
        "  - token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
        "  - tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UVL0b3puDYe",
        "outputId": "58b505c0-5612-40ec-e0c3-d2055e4c36a4"
      },
      "source": [
        "# ndarray 객체 생성\n",
        "import numpy as np\n",
        "dense = np.array([[3,0,1],[0,2,0]])\n",
        "dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P43qoYUNuDYe",
        "outputId": "3fd038ad-8bdb-4830-d69c-396375a24f04"
      },
      "source": [
        "# 희소 행렬 - COO 형식 : 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 행과 행의 위치를 별도의 배열로 \n",
        "# 저장\n",
        "# 희소 행렬 변환을 위해 scipy sparse 패키지를 이용\n",
        "from scipy import sparse\n",
        "data = np.array([3,1,2])\n",
        "row_pos = np.array([0,0,1]) # dense 2차원 배열에서 0이 아닌 데이터의 위치를 (row, col)로 표시\n",
        "col_pos = np.array([0,2,1])\n",
        "sparse_coo = sparse.coo_matrix((data,(row_pos,col_pos)))\n",
        "print(sparse_coo)\n",
        "# sparse_coo.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t3\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Hz-4rsuDYe",
        "outputId": "8e3491ba-78e8-4167-8e06-1db9cc28932f"
      },
      "source": [
        "# 희소 행렬 - CSR 형식 : COO 형식의 반복적인 위치 데이터를 사용하는 문제점을 보완. \n",
        "# 반복 제거(위치의 위치를 표기), 메모리 적게 들고 빠른 연산 가능\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                   [1,4,0,3,2,5],\n",
        "                   [0,6,0,3,0,0],\n",
        "                   [2,0,0,0,0,0],\n",
        "                   [0,0,0,7,0,8],\n",
        "                   [1,0,0,0,0,0]])\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "# COO 형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\n",
        "# sparse_coo = sparse.coo_matrix(dense2)\n",
        "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
        "# CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos,row_pos_ind))\n",
        "# sparse_csr = sparse.csr_matrix(dense2)\n",
        "print(sparse_coo)\n",
        "print(sparse_coo.toarray())\n",
        "print()\n",
        "print(sparse_csr)\n",
        "print(sparse_csr.toarray())\n",
        "# print(sparse_csr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2)\t1\n",
            "  (0, 5)\t5\n",
            "  (1, 0)\t1\n",
            "  (1, 1)\t4\n",
            "  (1, 3)\t3\n",
            "  (1, 4)\t2\n",
            "  (1, 5)\t5\n",
            "  (2, 1)\t6\n",
            "  (2, 3)\t3\n",
            "  (3, 0)\t2\n",
            "  (4, 3)\t7\n",
            "  (4, 5)\t8\n",
            "  (5, 0)\t1\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t5\n",
            "  (1, 0)\t1\n",
            "  (1, 1)\t4\n",
            "  (1, 3)\t3\n",
            "  (1, 4)\t2\n",
            "  (1, 5)\t5\n",
            "  (2, 1)\t6\n",
            "  (2, 3)\t3\n",
            "  (3, 0)\t2\n",
            "  (4, 3)\t7\n",
            "  (4, 5)\t8\n",
            "  (5, 0)\t1\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMFFAXOjuDYf",
        "outputId": "61f59c87-6400-4cca-f9b8-8cba03d758bc"
      },
      "source": [
        "# 밀집 행렬을 생성 파라미터로 입력하면 COO, CSR 희소 행렬로 생성\n",
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "                   [1,4,0,3,2,5],\n",
        "                   [0,6,0,3,0,0],\n",
        "                   [2,0,0,0,0,0],\n",
        "                   [0,0,0,7,0,8],\n",
        "                   [1,0,0,0,0,0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)\n",
        "print(coo)\n",
        "print(coo.toarray())\n",
        "print()\n",
        "print(csr)\n",
        "print(csr.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2)\t1\n",
            "  (0, 5)\t5\n",
            "  (1, 0)\t1\n",
            "  (1, 1)\t4\n",
            "  (1, 3)\t3\n",
            "  (1, 4)\t2\n",
            "  (1, 5)\t5\n",
            "  (2, 1)\t6\n",
            "  (2, 3)\t3\n",
            "  (3, 0)\t2\n",
            "  (4, 3)\t7\n",
            "  (4, 5)\t8\n",
            "  (5, 0)\t1\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t5\n",
            "  (1, 0)\t1\n",
            "  (1, 1)\t4\n",
            "  (1, 3)\t3\n",
            "  (1, 4)\t2\n",
            "  (1, 5)\t5\n",
            "  (2, 1)\t6\n",
            "  (2, 3)\t3\n",
            "  (3, 0)\t2\n",
            "  (4, 3)\t7\n",
            "  (4, 5)\t8\n",
            "  (5, 0)\t1\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuNOB8qguDYf",
        "outputId": "111e2a63-5429-4e00-8473-893c73deaf87"
      },
      "source": [
        "# DictVectorizer:문서에서 단어의 사용 빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치 벡터로 변환\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "v = DictVectorizer(sparse=False)\n",
        "D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]\n",
        "X = v.fit_transform(D)\n",
        "print(X)\n",
        "print()\n",
        "print(v.feature_names_)\n",
        "print(v.vocabulary_)\n",
        "print(v.transform({'C': 4, 'D': 3}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 0.]\n",
            " [0. 3. 1.]]\n",
            "\n",
            "['A', 'B', 'C']\n",
            "{'A': 0, 'B': 1, 'C': 2}\n",
            "[[0. 0. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkSxF8siuDYf",
        "outputId": "3218aa02-1cea-47ef-8609-7b196ad8a24c"
      },
      "source": [
        "# CountVectorizer\n",
        "# 문서를 토큰 리스트로 변환\n",
        "# 각 문서에서 토큰의 출현 빈도 카운트\n",
        "# 각 문서를 BOW 인코딩 벡터로 변환\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "    'The last document?',\n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus) # fit() 는 데이터를 모델에 학습시킬 때 사용.\n",
        "print(vect.get_feature_names()) # 토큰 리스트로 변환\n",
        "print(vect.vocabulary_) # 사전\n",
        "print()\n",
        "print(vect.transform(['This is the second document']).toarray()) # transform() 은 데이터를 알맞게 변형해 줌\n",
        "print()\n",
        "print(vect.transform(['Something completely new.']).toarray())\n",
        "print()\n",
        "print(vect.transform(corpus).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
            "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n",
            "\n",
            "[[0 1 0 1 0 0 1 1 0 1]]\n",
            "\n",
            "[[0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "[[0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 2 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liMitXW_uDYg",
        "outputId": "cfc4abd1-f1f9-440d-9792-5258b85a957a"
      },
      "source": [
        "# Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. 보통 영어의 관사나 접속사, 한국어의 조사 등\n",
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUcInLpuDYg",
        "outputId": "4d208136-c1bd-4c52-ff4b-7b3bd3159869"
      },
      "source": [
        "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': 0, 'second': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov_GiOEvuDYg",
        "outputId": "3008fd91-4e2d-4117-ed41-ffe2fa0fb505"
      },
      "source": [
        "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
        "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'t': 16,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 's': 15,\n",
              " ' ': 0,\n",
              " 'e': 6,\n",
              " 'f': 7,\n",
              " 'r': 14,\n",
              " 'd': 5,\n",
              " 'o': 13,\n",
              " 'c': 4,\n",
              " 'u': 17,\n",
              " 'm': 11,\n",
              " 'n': 12,\n",
              " '.': 1,\n",
              " 'a': 3,\n",
              " '?': 2,\n",
              " 'l': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAfXMmRVuDYg",
        "outputId": "cec1cc9d-a23a-4c8e-faca-cb7517c904dd"
      },
      "source": [
        "# \\w : word 를 표현하며 알파벳 + 숫자 + _ 중의 한 문자임을 의미\n",
        "\n",
        "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 2, 'the': 0, 'third': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsJOx_VJuDYh",
        "outputId": "67d624c2-0101-4a1c-d2e1-fbfca737b80c"
      },
      "source": [
        "# word_tokenize( )는 space 단위와 구두점(punctuation)을 기준으로 토큰화(Tokenize)\n",
        "\n",
        "import nltk\n",
        "\n",
        "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\anaconda3\\envs\\ca_ba\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 11,\n",
              " 'is': 5,\n",
              " 'the': 9,\n",
              " 'first': 4,\n",
              " 'document': 3,\n",
              " '.': 0,\n",
              " 'second': 8,\n",
              " 'and': 2,\n",
              " 'third': 10,\n",
              " 'one': 7,\n",
              " '?': 1,\n",
              " 'last': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUFm1BlSuDYh",
        "outputId": "41ce76af-213a-49e2-e51b-c0ea9d4dbfde"
      },
      "source": [
        "# n-그램은 단어장 생성에 사용할 토큰의 크기를 결정\n",
        "# 모노그램(1-그램)은 토큰 하나만 단어로 사용하며 바이그램(2-그램)은 두 개의 연결된 토큰을 하나의 단어로 사용\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 21,\n",
              " 'is': 5,\n",
              " 'the': 14,\n",
              " 'first': 3,\n",
              " 'document': 2,\n",
              " 'this is': 22,\n",
              " 'is the': 6,\n",
              " 'the first': 15,\n",
              " 'first document': 4,\n",
              " 'second': 11,\n",
              " 'the second': 17,\n",
              " 'second second': 13,\n",
              " 'second document': 12,\n",
              " 'and': 0,\n",
              " 'third': 19,\n",
              " 'one': 10,\n",
              " 'and the': 1,\n",
              " 'the third': 18,\n",
              " 'third one': 20,\n",
              " 'is this': 7,\n",
              " 'this the': 23,\n",
              " 'last': 8,\n",
              " 'the last': 16,\n",
              " 'last document': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArWSoU6muDYi",
        "outputId": "2369fb1b-863b-4f06-ef0c-1d99a940e996"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmZLEzRPuDYi",
        "outputId": "cd15936a-86d3-4bed-cf98-cd783af9f3f2"
      },
      "source": [
        "# max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성\n",
        "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
        "vect.vocabulary_, vect.stop_words_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'this': 3, 'is': 2, 'first': 1, 'document': 0},\n",
              " {'and', 'last', 'one', 'second', 'the', 'third'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqJYpm6-uDYi",
        "outputId": "ecdd1515-ba86-4536-d2a4-2bda240207b7"
      },
      "source": [
        "print(vect.transform(corpus).toarray())\n",
        "vect.transform(corpus).toarray().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1]\n",
            " [1 0 1 1]\n",
            " [0 0 0 0]\n",
            " [1 1 1 1]\n",
            " [1 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2, 3, 3], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Ayps1euDYj"
      },
      "source": [
        "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 \n",
        "모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소\n",
        "문서 d(document)와 단어 t 에 대해 다음과 같이 계산  \n",
        "\n",
        "tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
        "\n",
        "tf(d,t): term frequency. 특정한 단어의 빈도수  \n",
        "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수  \n",
        "n : 전체 문서의 수  \n",
        "df(t) : 단어  t 를 가진 문서의 수  \n",
        "idf(d,t)=log(n/(1+df(t))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGpbQNKluDYj",
        "outputId": "2d215d9b-6aa3-4fe3-965d-f919077496ee"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidv = TfidfVectorizer(max_df=4, min_df=2).fit(corpus)\n",
        "tfidv.transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.41250005, 0.59072194, 0.49035258, 0.49035258],\n",
              "       [0.51123153, 0.        , 0.60771799, 0.60771799],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.41250005, 0.59072194, 0.49035258, 0.49035258],\n",
              "       [1.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSKatsZzuDYk",
        "outputId": "5c8ed220-df46-407b-e093-ca6351ae5557"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화, english stop word 제거\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "def tokenize_text(text):\n",
        "\n",
        "    sentences = sent_tokenize(text) # 문장별 분리 토큰\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
        "    return word_tokens\n",
        "corpus = '''\n",
        "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile \n",
        "ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
        "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. \n",
        "It leaked -- like a genie out of a bottle -- from a lab in an accident. \n",
        "It took root at a wildlife market in Wuhan.\n",
        "'''\n",
        "\n",
        "word_tokens = tokenize_text(corpus)\n",
        "\n",
        "# stopwords 필터링을 통한 제거\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens =[]\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "print(all_tokens)\n",
        "print(type(all_tokens), len(all_tokens)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['vacuum', 'knowledge', 'origins', 'new', 'coronavirus', 'ravaging', 'world', 'provided', 'fertile', 'ground', 'manner', 'theories', '--', 'fantastic', ',', 'dubious', 'believable', '.'], ['bioweapon', 'manufactured', 'chinese', '.'], ['us', 'army', 'brought', 'virus', 'wuhan', '.'], ['leaked', '--', 'like', 'genie', 'bottle', '--', 'lab', 'accident', '.'], ['took', 'root', 'wildlife', 'market', 'wuhan', '.']]\n",
            "<class 'list'> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI8o27CluDYk",
        "outputId": "643fc2cc-880f-41f0-fe44-c8f1d63944b9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [ '''\n",
        "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile \n",
        "ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
        "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. \n",
        "It leaked -- like a genie out of a bottle -- from a lab in an accident. \n",
        "It took root at a wildlife market in Wuhan.\n",
        "''']\n",
        "\n",
        "\n",
        "vect = CountVectorizer(stop_words=\"english\")\n",
        "vect.fit(corpus) # fit() 는 데이터를 모델에 학습시킬 때 사용.\n",
        "print(vect.get_feature_names())\n",
        "print(vect.vocabulary_)\n",
        "\n",
        "print(vect.transform(['This is the second document']).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
            "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwepUrQAuDYk",
        "outputId": "cb5f473d-b65d-40a3-f568-34620b22a0b7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidv = TfidfVectorizer(stop_words=\"english\").fit(corpus)\n",
        "print(tfidv.get_feature_names())\n",
        "print(tfidv.vocabulary_)\n",
        "print()\n",
        "print(tfidv.transform(['This is the second document']).toarray())\n",
        "tfidv.transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
            "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n",
            "\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.16903085, 0.16903085, 0.16903085, 0.16903085,\n",
              "        0.16903085, 0.3380617 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqzo7YP6uDYk"
      },
      "source": [
        "### 형태소 : 의미를 가지는 요소로서는 더 이상 분석할 수 없는 가장 작은 말의 단위\n",
        "### KoNLPy는 시중에 공개된 hannanum, kkma, okt, komoran, mecab 다섯개 형태소 분석기를 한꺼번에 묶어서 편리하게 사용할 수 있도록 한 패키지\n",
        "### okt\n",
        "- morphs(phrase, norm=False, stem=False)\\\n",
        "  Parse phrase to morphemes.\n",
        "- nouns(phrase)  \n",
        "- phrases(phrase)  \n",
        "- pos(phrase, norm=False, stem=False, join=False)\\\n",
        "  매개 변수:\\\n",
        "  norm -- If True, normalize tokens.\\\n",
        "  stem -- If True, stem tokens.\\\n",
        "  join -- If True, returns joined sets of morph and tag\n",
        " \n",
        "- 파싱(Parsing)\n",
        " - 일련의 문자열을 의미있는 token(어휘 분석의 단위)으로 분해하고 그것들로 이루어진 Parse tree를 만드는 과정\n",
        " - 어떤 문장을 분석하거나 문법적 관계를 해석하는 행위\n",
        " - 프로그램을  compile하는 과정에서 특정 프로그래밍 언어가 제시하는 문법을 잘 지켜서 작성하였는지 compiler가 검사하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA2hok9SuDYk"
      },
      "source": [
        "# KoNLPy 설치\n",
        "# Java 환경 세팅, JPype1 다운로드 받고 설치(conda -c conda-forge jpype1)\n",
        "# pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kVO-O5YvIhD",
        "outputId": "11315d6a-0449-4edf-9907-ab912fb0530c"
      },
      "source": [
        "!pip install jpype1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jpype1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\r\u001b[K     |▊                               | 10kB 29.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 34.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 17.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 20.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 22.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61kB 24.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 92kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 102kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 122kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 153kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 163kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 174kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 184kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 194kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 204kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 225kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 235kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 245kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 256kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 266kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 276kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 286kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 296kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 307kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 317kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 327kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 337kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 348kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 358kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 368kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 378kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 389kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 399kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 409kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 419kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 430kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 440kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 450kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jpype1) (3.7.4.3)\n",
            "Installing collected packages: jpype1\n",
            "Successfully installed jpype1-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHIFtsgxwTdU",
        "outputId": "e958bb39-1fee-41d0-a8a2-07389c5595c9"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAftrwGouDYk",
        "outputId": "d8de4500-a0f7-4613-f2ba-2c449d1d9672"
      },
      "source": [
        "# 형태소 분석으로 문장을 단어로 분할\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs('단독입찰보다 복수입찰의 경우'))\n",
        "print()\n",
        "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
        "print()\n",
        "print(okt.pos('아름다운 꽃과 파란 하늘'))\n",
        "print()\n",
        "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))\n",
        "print()\n",
        "# norm 옵션 : '되나욬' 처럼 작성시 '되나요'로 변환\n",
        "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))\n",
        "print()\n",
        "# stem 옵션 : '되나욬' 처럼 작성시 '되다'로 원형을 찾아줌\n",
        "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
        "print()\n",
        "# join 옵션 : joined sets of morph and tag\n",
        "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True, join=True))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
            "\n",
            "['항공기', '체계', '종합', '개발', '경험']\n",
            "\n",
            "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n",
            "\n",
            "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
            "\n",
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
            "\n",
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
            "\n",
            "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-9kZjuDYl",
        "outputId": "c69c8706-db0e-4c65-fb17-229e8f0940ea"
      },
      "source": [
        "# 형용사인 품사만 선별해 리스트에 담기\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "morph = okt.pos('아름다운 꽃과 파란 하늘')\n",
        "print(morph)\n",
        "adj_list = []\n",
        "for word, tag in morph:\n",
        "    if tag == 'Adjective':\n",
        "        adj_list.append(word)\n",
        "print(adj_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]\n",
            "['아름다운']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C48mHXsFuDYl",
        "outputId": "7524ee8c-4e19-4040-fdaa-c3e0dd7fd016"
      },
      "source": [
        "sentence_tag=[]\n",
        "sentences = ['아름다운 꽃과 파란 하늘']\n",
        "\n",
        "for sentence in sentences:\n",
        "    morph = okt.pos(sentence)\n",
        "    sentence_tag.append(morph)\n",
        "    print(sentence_tag)\n",
        "#     print(morph)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('아름다운', 'Adjective'), ('꽃', 'Noun'), ('과', 'Josa'), ('파란', 'Noun'), ('하늘', 'Noun')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqHqr1zQuDYl",
        "outputId": "0637233f-5f41-4559-fbdf-be5fd261bc36"
      },
      "source": [
        "# okt.morphs, okt.pos\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "malist1 = okt.nouns('나는 오늘 방콕에 가고싶다.')\n",
        "malist2 = okt.pos('나는 오늘 방콕에 갔다.', norm=True, stem=True)\n",
        "malist3 = okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.')\n",
        "malist4 = okt.pos('나는 오늘도 장에 가고싶다.', norm=True, stem=True, join=True)\n",
        "malist5 = okt.pos('나는 오늘 장에 가고싶을깤ㅋㅋ?', norm=True, stem=True)\n",
        "print('명사')\n",
        "print(malist1)\n",
        "print('원형')\n",
        "print(malist2)\n",
        "print('형태소')\n",
        "print(malist3)\n",
        "print('형태소/태그')\n",
        "print(malist4)\n",
        "print('정규화, 원형')\n",
        "print(malist5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "명사\n",
            "['나', '오늘', '방콕']\n",
            "원형\n",
            "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('방콕', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('.', 'Punctuation')]\n",
            "형태소\n",
            "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
            "형태소/태그\n",
            "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가다/Verb', './Punctuation']\n",
            "정규화, 원형\n",
            "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftNZUyGfuDYl"
      },
      "source": [
        "#### 텍스트 분류\n",
        "- 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측\n",
        "- 텍스트를 피처 벡터화로 변환, 희소 행렬로 만들고 로지스틱 회귀를 이용해 분류 수행\n",
        "- Count 기반 과 TF-IDF 기반의 벡터화를 각각 적용, 성능 비교\n",
        "- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼파라미터 튜닝을 일괄적으로 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0kgQyAXuDYl",
        "outputId": "33a90184-4637-4af8-a92d-e41c75e616cb"
      },
      "source": [
        "# 데이터 가져오기\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news_data = fetch_20newsgroups(subset='all',random_state=0)\n",
        "news_data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmywbRSFuDYl",
        "outputId": "9bffad6a-f898-442d-d5bd-c78223fdc676"
      },
      "source": [
        "import pandas as pd\n",
        "print(news_data.target)\n",
        "a = pd.Series(news_data.target).unique()\n",
        "# a.sort()\n",
        "# a\n",
        "sorted(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6  1 15 ...  0  5  8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DKQCaPUuDYm"
      },
      "source": [
        "news_data.DESCR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UU2P1a7uDYm",
        "outputId": "95d8572b-b6c8-4f38-ef83-38d76a842c6b"
      },
      "source": [
        "# 텍스트 정규화\n",
        "# 뉴스그룹 기사내용을 제외하고 다른 정보 제거\n",
        "# 제목, 소속, 이메일 등 헤더와 푸터 정보들은 분류의 타겟 클래스 값과 유사할 수 있음\n",
        "\n",
        "train_news = fetch_20newsgroups(subset='train', remove=('header','footer','quotes'), random_state=0)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "test_news = fetch_20newsgroups(subset='test', remove=('header','footer','quotes'), random_state=0)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 7532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-vNYg2WuDYm",
        "outputId": "2f8e602f-ba50-4b48-85f9-61ce6cbe13ea"
      },
      "source": [
        "import pandas as pd\n",
        "print(news_data.target_names)\n",
        "print(pd.Series(y_test).value_counts().sort_index())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "0     319\n",
            "1     389\n",
            "2     394\n",
            "3     392\n",
            "4     385\n",
            "5     395\n",
            "6     390\n",
            "7     396\n",
            "8     398\n",
            "9     397\n",
            "10    399\n",
            "11    396\n",
            "12    393\n",
            "13    396\n",
            "14    394\n",
            "15    398\n",
            "16    364\n",
            "17    376\n",
            "18    310\n",
            "19    251\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyIlrIyLuDYm",
        "outputId": "a773c210-b9e4-4279-b383-32bd9246be18"
      },
      "source": [
        "# 피처 벡터화 변환\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "# 학습 데이터로 fit( )된 CountVectorizer를 이용, 테스트 데이터 피처 벡터화 변환(피처 개수가 동일해야 함) \n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
        "print(X_train_cnt_vect.shape)\n",
        "print(X_test_cnt_vect.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 120756)\n",
            "(7532, 120756)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8oth5PtuDYm",
        "outputId": "5c4ae54d-874d-4d94-a943-86a59e702179"
      },
      "source": [
        "# 머신러닝 모델 학습/예측/평가\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_cnt_vect, y_train)\n",
        "lr_pred = lr_clf.predict(X_test_cnt_vect)\n",
        "print(accuracy_score(y_test,lr_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7553106744556559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHRs6dDOuDYm",
        "outputId": "cbbb5f24-aed7-4b5e-ab68-83a8952cf331"
      },
      "source": [
        "# 피처 벡터화 변환 : TF-IDF 벡터화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "# 학습 / 평가\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, lr_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7841210833775889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XedFP2QTuDYm",
        "outputId": "d4bc595b-3160-40d3-e8e3-58b0e8bdae02"
      },
      "source": [
        "# stop words 필터링 추가, ngram을 기본 (1,1)에서 (1,2)로 max_df=300으로 변경해 피처 벡터화 적용\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2),max_df=300)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, lr_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.774429102496017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKyKkVBuDYn"
      },
      "source": [
        "#### Q. Random Forest, SVM을 적용 뉴스그룹에 대한 분류 예측을 수행하세요.\n",
        "svm_clf = svm.SVC(kernel='rbf')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBBQ4SoPuDYn",
        "outputId": "70c012b9-311a-4399-965b-4b59c2e72d26"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(X_train_tfidf_vect,y_train)\n",
        "rf_pred = rf_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test,rf_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7677907594264471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAe5KOreuDYn"
      },
      "source": [
        "from sklearn import svm\n",
        "svm_clf = svm.SVC(kernel='rbf',random_state=0)\n",
        "svm_clf.fit(X_train_tfidf_vect,y_train)\n",
        "svm_pred = svm_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, svm_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G12cL04BuDYn"
      },
      "source": [
        "# DT\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "dt_clf.fit(X_train_tfidf_vect,y_train)\n",
        "dt_pred = dt_clf(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test,dt_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W89u9AAQuDYn"
      },
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train_tfidf_vect,y_train)\n",
        "knn_pred = knn_clf.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test,knn_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8o5axMWuDYn"
      },
      "source": [
        "# 30분 이상 걸림\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정.\n",
        "params = {'C':[5,10]}\n",
        "gcv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, \\\n",
        "                          scoring='accuracy',verbose = 1)\n",
        "gcv_lr.fit(X_train_tfidf_vect, y_train)\n",
        "print(gcv_lr.best_params_)\n",
        "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가.\n",
        "lr_pred = gcv_lr.predict(X_test_tfidf_vect)\n",
        "print(accuracy_score(y_test, lr_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE03a5EvuDYn"
      },
      "source": [
        "#### 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6WoYSaJuDYn"
      },
      "source": [
        "# Pipeline 사용 \n",
        "from sklearn.pipeline import Pipeline\n",
        "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 \n",
        "# lr_clf 객체명으로 생성하는 Pipeline생성\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english',\\\n",
        "                                   ngram_range=(1,2),max_df=300)),\\\n",
        "    ('lr_clf', LogisticRegression(C=10))])\n",
        "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), \n",
        "# predict( )가 필요 없음. \n",
        "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 \n",
        "# ML 학습/예측이 가능. \n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVaq20__uDYo"
      },
      "source": [
        "# 30분 이상 소요\n",
        "# GridSearchCV와의 결합\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
        "    ('lr_clf', LogisticRegression())\n",
        "])\n",
        "# 하이퍼파라미터명이 개게 변수명과 결합 : 피처 벡터화 객체 파라미터와 Estimator 객체의 하이퍼파라미터 구별하기 위함\n",
        "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
        "         'tfidf_vect__max_df':[100,300,700],\n",
        "         'lr_clf__C':[1,5,10]}\n",
        "\n",
        "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, \\\n",
        "                           scoring = 'accuracy', verbose = 1)\n",
        "grid_cv_pipe.fit(X_train,y_train)\n",
        "print(grid_cv_pipe.best_params_, grid_cv_pripe.best_score_)\n",
        "\n",
        "pred = grid_cv_pipe.predict(X_test)\n",
        "print(accuracy_score(y_test,pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpWPz9D1uDYo"
      },
      "source": [
        "#### 감성분석\n",
        "- 감성분석은 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성 수치를 계산하는 방법을 이용\n",
        "- 감성 지수는 긍정 감성 지수와 부정 감성 지수로 구성되며 이들 지수를 합산해 긍정 또는 부정 감성을 결정\n",
        "- 지도 학습은 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습을 수행한 뒤 이를 기반으로 다른 데이터의 감성 분석을 예측하는 방법\n",
        "- 비지도 학습은 'Lexicon'이라는 일종의 감성 어휘 사전을 이용. Lexicon의 감성 분석을 위한 용어와 문맥에 대한 다양한 정보를 이용해 문서의 긍정적 부정적 감성 여부를 판단"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS1TCptDuDYo",
        "outputId": "0db4e794-84fc-4332-9d4f-b61372b89037"
      },
      "source": [
        "import pandas as pd\n",
        "review_df = pd.read_csv('dataset/labeledTrainData.tsv',header=0,sep='\\t',quoting=3)\n",
        "print(review_df.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id  sentiment                                             review\n",
            "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
            "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
            "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgt0_lDbuDYo",
        "outputId": "45988144-169c-425f-b471-bc0d96f5b5f0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "rf = review_df[:5]\n",
        "rf\n",
        "rf['review_a'] = lemma.lemmatize('rf.review','a')\n",
        "rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "      <th>review_a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "      <td>rf.review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "      <td>rf.review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "      <td>rf.review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "      <td>rf.review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "      <td>rf.review</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  sentiment                                             review  \\\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
              "\n",
              "    review_a  \n",
              "0  rf.review  \n",
              "1  rf.review  \n",
              "2  rf.review  \n",
              "3  rf.review  \n",
              "4  rf.review  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY9pgQBIuDYo"
      },
      "source": [
        "review_df.review[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJXXMz-yuDYo"
      },
      "source": [
        "# df/series에서 str 적용 문자열 연산 수행\n",
        "# 파이썬의 정규 표현식 모듈인 re를 이용하여 영어 문자열이 아닌 문자는 모두 공백으로 변환\n",
        "# re.sub('패턴', '바꿀문자열', '문자열', 바꿀횟수)\n",
        "import re\n",
        "review_df.review = review_df.review.str.replace('<br />',' ')\n",
        "review_df.review = review_df.review.apply(lambda x : re.sub('[^a-zA-Z]',' ',x))\n",
        "review_df.review[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeJVsbQruDYo",
        "outputId": "e18b585d-fb8b-4598-b0ab-82e5410c4227"
      },
      "source": [
        "# 학습용/평가용 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "class_df = review_df.sentiment\n",
        "feature_df = review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(feature_df,class_df, test_size=0.3, random_state=10)\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17500, 1) (7500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9S2cGnpuDYp",
        "outputId": "574ccd5b-1f48-4d81-c1fc-127a1d8c2193"
      },
      "source": [
        "# 학습 및 평가 - Count 기반 피처 벡터화\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([('cnt_vect', CountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
        "                    ('lr_clf', LogisticRegression(C=10))])\n",
        "pipeline.fit(X_train['review'],y_train)\n",
        "pred = pipeline.predict(X_test['review'])\n",
        "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
        "print(accuracy_score(y_test,pred), roc_auc_score(y_test,pred_probs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8861333333333333 0.9515652770296994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhREgo5ZuDYp"
      },
      "source": [
        "# 학습 및 평가 - tfidf 기반 피처 벡터화\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english',ngram_range=(1,2))),\n",
        "                    ('lr_clf', LogisticRegression(C=10))])\n",
        "pipeline.fit(X_train['review'],y_train)\n",
        "pred = pipeline.predict(X_test['review'])\n",
        "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
        "print(accuracy_score(y_test,pred), roc_auc_score(y_test,pred_probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqfFadUnuDYp"
      },
      "source": [
        "#  비지도학습 기반 감성 분석"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2cYzS7OuDYp",
        "outputId": "ffcf2363-eb37-4a90-8a3e-a79f82988158"
      },
      "source": [
        "# 한글 텍스트 처리\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv('./dataset/ratings_train.txt',sep='\\t')\n",
        "test_df = pd.read_csv('./dataset/ratings_test.txt', sep='\\t')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRemIggPuDYp",
        "outputId": "8f2e7444-2ffc-40b2-9472-3247e712bd77"
      },
      "source": [
        "# label 비율 : 1이 긍정, 0이 부정\n",
        "train_df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75173\n",
              "1    74827\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSm35hnGuDYp",
        "outputId": "5f9953de-6efe-431f-cc2c-23a7fc141b43"
      },
      "source": [
        "train_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0\n",
              "document    5\n",
              "label       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mhwoCkHuDYp"
      },
      "source": [
        "# r 문자는 raw string으로 백슬래시 문자를 해석하지 않고 남겨두기 때문에 정규표현식과 같은 곳에 유용\n",
        "import re\n",
        "train_df = train_df.fillna(' ')\n",
        "train_df['document'] = train_df['document'].apply(lambda x : re.sub(r\"\\d+\",\" \",x))\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df.document = test_df.document.apply(lambda x : re.sub(r\"\\d+\", \" \",x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBUirLMUuDYq"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import numpy as np\n",
        "\n",
        "okt = Okt()\n",
        "# 토큰화\n",
        "def okt_tokenizer(text):\n",
        "    tokens_ko = okt.morphs(text)\n",
        "    return tokens_ko\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=okt_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df.document)\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df.document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR_wV5O3uDYq",
        "outputId": "1c378374-20cc-4ca2-808c-b07ba3335756"
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "params = {'C':[1,3.5,4.5,5.5,10]}\n",
        "grid_cv = GridSearchCV(lr_clf,param_grid=params,cv=3,scoring='accuracy',verbose=1)\n",
        "grid_cv.fit(tfidf_matrix_train,train_df.label)\n",
        "print(grid_cv.best_params_, round(grid_cv.best_score_,4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   49.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 3.5} 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3s1iFM_uDYq",
        "outputId": "fba44415-99aa-494e-cf61-e5d6cd147792"
      },
      "source": [
        "# 시간 소요\n",
        "from sklearn.metrics import accuracy_score\n",
        "tfidf_matrix_test = tfidf_vect.transform(test_df.document)\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "preds = best_estimator.predict(tfidf_matrix_test)\n",
        "print(accuracy_score(test_df.label, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.86186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUGzNd2XuDYq"
      },
      "source": [
        "#### 토픽 모델링\n",
        "- 머신러닝 기반의 토픽 모델링을 적용해 문서 집합에 숨어 있는 주제를 찾아냄\n",
        "- 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것에 반해 머신러닝 기반의 토픽 모델링은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출\n",
        "- LSA(Latent Sementic Analysis) 와 LDA(Latent Dirichlet Allocation) 기법\n",
        " - LSA는 단어-문서행렬(Word-Document Matrix), 단어-문맥행렬(window based co-occurrence matrix) 등 입력 데이터에 특이값 분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우면서 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론\n",
        " - LDA는 미리 알고 있는 주제별 단어수 분포를 바탕으로, 주어진 문서에서 발견된 단어수 분포를 분석, 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjm4zzmYuDYq",
        "outputId": "f1db99bf-1fd8-410e-de5e-6acd8d856507"
      },
      "source": [
        "news_data.keys()\n",
        "print(news_data.target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4BTTUWuuDYq"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x', \\\n",
        "        'talk.politics.mideast', 'soc.religion.christian','sci.electronics', 'sci.med']\n",
        "\n",
        "news_df = fetch_20newsgroups(subset='all',remove=('header','footers','quotes'),\n",
        "                            categories=cats,random_state=1)\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english',\\\n",
        "                             ngram_range=(1,2))\n",
        "feat_vect = count_vect.fit_transform(news_df.data)\n",
        "print(feat_vect.shape)\n",
        "lda = LatentDirichletAllocation(n_components=8, random_state=1)\n",
        "lda.fit(feat_vect) \n",
        "print(lda.components_.shape)\n",
        "lda.components_[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoBip8LhuDYq",
        "outputId": "4b7ce50f-b4de-4164-c923-a179f0891d90"
      },
      "source": [
        "feature_names = count_vect.get_feature_names()\n",
        "len(feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_au10O7uDYr",
        "outputId": "357fa350-08d1-46d1-d407-a76eb4ac4fb9"
      },
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_index, topic in enumerate(model.components_):\n",
        "        print('Topic #', topic_index)\n",
        "        \n",
        "        topic_word_indexes = topic.argsort()[::-1]\n",
        "        top_indexes=topic_word_indexes[:no_top_words]\n",
        "        \n",
        "        feature_concat = \" \".join([feature_names[i] for i in top_indexes])\n",
        "        print(feature_concat)\n",
        "feature_names = count_vect.get_feature_names()\n",
        "display_topics(lda,feature_names,15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic # 0\n",
            "organization year edu good game time 10 medical years health new use team com better\n",
            "Topic # 1\n",
            "israel edu israeli jewish 00 organization jews arab apr 1993 ibm 04 03 gmt 02\n",
            "Topic # 2\n",
            "armenian armenians turkish people turkey serdar armenia argic muslim serdar argic genocide world government said turks\n",
            "Topic # 3\n",
            "god jesus church christ sin faith organization netcom bible love lord man com paul christian\n",
            "Topic # 4\n",
            "edu graphics computer organization cs science mail information university gov list send computer science data nasa\n",
            "Topic # 5\n",
            "don people just know like think say time way organization said did right want going\n",
            "Topic # 6\n",
            "file use image program window jpeg version server display available windows color files mit using\n",
            "Topic # 7\n",
            "edu organization posting host nntp nntp posting posting host com university distribution reply organization university ca world distribution world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XThwNWcuDYr"
      },
      "source": [
        "#### 문서 군집화\n",
        "- 비슷한 텍스트 구성의 문서를 군집화하여 같은 카테고리 소속으로 분류\n",
        "- 학습 데이터 세트가 필요없는 비지도학습 기반으로 동작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tKObWQDuDYr",
        "outputId": "7336f590-1d28-42c1-b013-b5be6b870d3e"
      },
      "source": [
        "# Opinion Review 데이터 세트를 이용한 문서 군집화 수행\n",
        "# 51개 텍스트 파일로 구성, 호텔, 자동차, 전자제품 사이트에서 가져온 리뷰 문서\n",
        "# glob 모듈의 glob 함수는 path로 지정한 디렉토리 밑에 있는 모든 .data 파일들의 파일명을 리스트로 반환\n",
        "# os.path.join('C:\\Tmp', 'a', 'b') -> \"C:\\Tmp\\a\\b\" 경로를 병합하여 새 경로 생성\n",
        "import pandas as pd\n",
        "import glob, os\n",
        "\n",
        "path = 'dataset/topics'\n",
        "all_files = glob.glob(os.path.join(path,'*.data'))\n",
        "\n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "# 개별 파일들의 파일명은 filename_list 리스트로 취합, \n",
        "# 개별 파일들의 파일내용은 DataFrame로딩 후 다시 string으로 변환하여 opinion_text 리스트로 취합 \n",
        "for file_ in all_files:\n",
        "    df=pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "    filename_list.append(filename)\n",
        "\n",
        "    opinion_text.append(df.to_string())\n",
        "document_df = pd.DataFrame({'filename':filename_list,'opinion_text':opinion_text})\n",
        "document_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>opinion_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
              "      <td>, and is very, very acc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
              "      <td>The room was not overly big, but clean and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>battery-life_amazon_kindle</td>\n",
              "      <td>After I plugged it in to my USB hub on my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>battery-life_ipod_nano_8gb</td>\n",
              "      <td>short battery life  I moved up from a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>battery-life_netbook_1005ha</td>\n",
              "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  \\\n",
              "0   accuracy_garmin_nuvi_255W_gps   \n",
              "1  bathroom_bestwestern_hotel_sfo   \n",
              "2      battery-life_amazon_kindle   \n",
              "3      battery-life_ipod_nano_8gb   \n",
              "4     battery-life_netbook_1005ha   \n",
              "\n",
              "                                        opinion_text  \n",
              "0                         , and is very, very acc...  \n",
              "1      The room was not overly big, but clean and...  \n",
              "2      After I plugged it in to my USB hub on my ...  \n",
              "3           short battery life  I moved up from a...  \n",
              "4      6GHz 533FSB cpu, glossy display, 3, Cell 2...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiu_i1dRuDYr",
        "outputId": "fa053447-a5a9-4cad-ab1b-70577b85c21d"
      },
      "source": [
        "# nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldr7u45VuDYr"
      },
      "source": [
        "# tokenizer는 LemNormalize() 함수를 이용\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L07hzqEbuDYs"
      },
      "source": [
        "# 피처 벡터화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\\\n",
        "                            ngram_range=(1,2), min_df=0.05, max_df=0.85)\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIvlWbn9uDYs",
        "outputId": "38649500-2575-4efb-ab08-f2f1bebc1f15"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>opinion_text</th>\n",
              "      <th>cluster_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
              "      <td>, and is very, very acc...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
              "      <td>The room was not overly big, but clean and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>battery-life_amazon_kindle</td>\n",
              "      <td>After I plugged it in to my USB hub on my ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>battery-life_ipod_nano_8gb</td>\n",
              "      <td>short battery life  I moved up from a...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>battery-life_netbook_1005ha</td>\n",
              "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  \\\n",
              "0   accuracy_garmin_nuvi_255W_gps   \n",
              "1  bathroom_bestwestern_hotel_sfo   \n",
              "2      battery-life_amazon_kindle   \n",
              "3      battery-life_ipod_nano_8gb   \n",
              "4     battery-life_netbook_1005ha   \n",
              "\n",
              "                                        opinion_text  cluster_label  \n",
              "0                         , and is very, very acc...              3  \n",
              "1      The room was not overly big, but clean and...              0  \n",
              "2      After I plugged it in to my USB hub on my ...              4  \n",
              "3           short battery life  I moved up from a...              4  \n",
              "4      6GHz 533FSB cpu, glossy display, 3, Cell 2...              4  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGs7U3-UuDYs"
      },
      "source": [
        "document_df[document_df.cluster_label==0].sort_values(by='filename')\n",
        "document_df[document_df.cluster_label==1].sort_values(by='filename')\n",
        "document_df[document_df.cluster_label==2].sort_values(by='filename')\n",
        "document_df[document_df.cluster_label==3].sort_values(by='filename')\n",
        "document_df[document_df.cluster_label==4].sort_values(by='filename')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBS3sIL9uDYs"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label\n",
        "document_df.sort_values(by='cluster_label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcZcgcJRuDYs",
        "outputId": "7248d3b8-360f-41c5-a995-d1d4b64e84ad"
      },
      "source": [
        "# 군집별 핵심 단어 추출\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "print(cluster_centers.shape)\n",
        "print(cluster_centers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2409)\n",
            "[[0.0174987  0.         0.         ... 0.         0.         0.00452955]\n",
            " [0.         0.00170335 0.0025537  ... 0.0032582  0.00349413 0.        ]\n",
            " [0.         0.00152566 0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrhG200duDYs",
        "outputId": "3d1adaf4-afa9-49b9-a399-dc2701057e3c"
      },
      "source": [
        "# 군집별 top n 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명들을 반환함. \n",
        "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
        "    cluster_details = {}\n",
        "    \n",
        "    # cluster_centers array 의 값이 큰 순으로 정렬된 index 값을 반환\n",
        "    # 군집 중심점(centroid)별 할당된 word 피처들의 거리값이 큰 순으로 값을 구하기 위함.  \n",
        "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
        "    \n",
        "    #개별 군집별로 iteration하면서 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명 입력\n",
        "    for cluster_num in range(clusters_num):\n",
        "        # 개별 군집별 정보를 담을 데이터 초기화. \n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "        \n",
        "        # cluster_centers_.argsort()[:,::-1] 로 구한 index 를 이용하여 top n 피처 단어를 구함. \n",
        "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
        "        top_features = [ feature_names[ind] for ind in top_feature_indexes ]\n",
        "        \n",
        "        # top_feature_indexes를 이용해 해당 피처 단어의 중심 위치 상댓값 구함 \n",
        "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
        "        \n",
        "        # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값, 그리고 해당 파일명 입력\n",
        "        cluster_details[cluster_num]['top_features'] = top_features\n",
        "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
        "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
        "        filenames = filenames.values.tolist()\n",
        "        cluster_details[cluster_num]['filenames'] = filenames\n",
        "        \n",
        "    return cluster_details\n",
        "\n",
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print('####### Cluster {0}'.format(cluster_num))\n",
        "        print('Top features:', cluster_detail['top_features'])\n",
        "        print('Reviews 파일명 :',cluster_detail['filenames'][:7])\n",
        "        print('==================================================')\n",
        "        \n",
        "feature_names = tfidf_vect.get_feature_names()\n",
        "\n",
        "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df,\\\n",
        "                                  feature_names=feature_names, clusters_num=3, top_n_features=10 )\n",
        "print_cluster_details(cluster_details)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####### Cluster 0\n",
            "Top features: ['screen', 'battery', 'life', 'battery life', 'keyboard', 'kindle', 'size', 'button', 'easy', 'voice']\n",
            "Reviews 파일명 : ['accuracy_garmin_nuvi_255W_gps', 'battery-life_amazon_kindle', 'battery-life_ipod_nano_8gb', 'battery-life_netbook_1005ha', 'buttons_amazon_kindle', 'directions_garmin_nuvi_255W_gps', 'display_garmin_nuvi_255W_gps']\n",
            "==================================================\n",
            "####### Cluster 1\n",
            "Top features: ['room', 'hotel', 'service', 'location', 'staff', 'food', 'clean', 'bathroom', 'parking', 'room wa']\n",
            "Reviews 파일명 : ['bathroom_bestwestern_hotel_sfo', 'food_holiday_inn_london', 'food_swissotel_chicago', 'free_bestwestern_hotel_sfo', 'location_bestwestern_hotel_sfo', 'location_holiday_inn_london', 'parking_bestwestern_hotel_sfo']\n",
            "==================================================\n",
            "####### Cluster 2\n",
            "Top features: ['interior', 'seat', 'mileage', 'comfortable', 'car', 'gas', 'gas mileage', 'comfort', 'ride', 'performance']\n",
            "Reviews 파일명 : ['comfort_honda_accord_2008', 'comfort_toyota_camry_2007', 'gas_mileage_toyota_camry_2007', 'interior_honda_accord_2008', 'interior_toyota_camry_2007', 'mileage_honda_accord_2008', 'performance_honda_accord_2008']\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LwfAuiYuDYs"
      },
      "source": [
        "#### 문서와 문서 간의 유사도 비교\n",
        "- 코사인 유사도는 벡터와 벡터 간의 유사도 비교시 벡터의 상호 방향성이 얼마나 유사한지에 기반\\\n",
        "  즉 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용\n",
        "- 희소 행렬 기반에서 문서와 문서 벡터간의 크기에 기반한 유사도 지표는 정확도가 떨어지기가 쉬움\n",
        "- 또한 문서가 매우 긴 경우 단어의 빈도수가 더 많을 것이므로 빈도수에만 기반해서는 공정한 비교가 어려움\\\n",
        "  A문서에서 '머신러닝' 단어가 5번 언급되고 B문서에서는 3번 언급되었을 때 A문서가 B문서보다 10배 이상 크다면 오히려 B문서가  더 관련된 문서라고 볼 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5c460VSuDYt",
        "outputId": "d5eb7cca-12ee-44a7-bed1-1d910832cbb4"
      },
      "source": [
        "# 서로간의 문서 유사도를 코사인 유사도 기반으로 도출\n",
        "import numpy as np\n",
        "def cos_similarity(v1,v2):\n",
        "    dot_product = np.dot(v1,v2)\n",
        "    l2_norm = (np.sqrt(sum(np.square(v1)))*np.sqrt(sum(np.square(v2))))\n",
        "    similarity = dot_product / l2_norm\n",
        "    return similarity\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc_list = ['WeWork is suing SoftBank for abandoning a $3 billion share buyout ' ,\n",
        "            'WeWork is suing SoftBank ',\n",
        "            'SoftBank listed several ways that WeWork failed to fulfill conditions required to complete the tender offers']\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nswdAukuDYt",
        "outputId": "56431864-cbae-4e85-d679-d91b0bd8fb40"
      },
      "source": [
        "# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector  추출\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
        "similarity_simple = cos_similarity(vect1, vect2 )\n",
        "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 1, 문장 2 Cosine 유사도: 0.520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5pn-HTSuDYt",
        "outputId": "646efb9b-28ae-494d-9143-78c8aba0d626"
      },
      "source": [
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect1, vect3 )\n",
        "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
        "\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect2, vect3 )\n",
        "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 1, 문장 3 Cosine 유사도: 0.065\n",
            "문장 2, 문장 3 Cosine 유사도: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn1pM-XtuDYt",
        "outputId": "69669de4-e222-43f6-a2fc-d7dce990a5cd"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print()\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)\n",
        "print()\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple , feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print('shape:',similarity_simple_pair.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.52014158 0.06521183]]\n",
            "\n",
            "[[0.52014158 0.06521183]]\n",
            "\n",
            "[[1.         0.52014158 0.06521183]\n",
            " [0.52014158 1.         0.12537324]\n",
            " [0.06521183 0.12537324 1.        ]]\n",
            "shape: (3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZzXzPGGuDYt"
      },
      "source": [
        "# Opinion Review 데이터 셋을 이용한 문서 유사도 측정\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6tIvKDuDYt",
        "outputId": "047b269a-dc07-4f5b-eddc-297e4060568f"
      },
      "source": [
        "import pandas as pd\n",
        "import glob ,os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "path = './dataset/topics'\n",
        "all_files = glob.glob(os.path.join(path, \"*.data\"))     \n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in all_files:\n",
        "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "    filename_list.append(filename)\n",
        "    opinion_text.append(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
        "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label\n",
        "document_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>opinion_text</th>\n",
              "      <th>cluster_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
              "      <td>, and is very, very acc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
              "      <td>The room was not overly big, but clean and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>battery-life_amazon_kindle</td>\n",
              "      <td>After I plugged it in to my USB hub on my ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>battery-life_ipod_nano_8gb</td>\n",
              "      <td>short battery life  I moved up from a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>battery-life_netbook_1005ha</td>\n",
              "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 2...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         filename  \\\n",
              "0   accuracy_garmin_nuvi_255W_gps   \n",
              "1  bathroom_bestwestern_hotel_sfo   \n",
              "2      battery-life_amazon_kindle   \n",
              "3      battery-life_ipod_nano_8gb   \n",
              "4     battery-life_netbook_1005ha   \n",
              "\n",
              "                                        opinion_text  cluster_label  \n",
              "0                         , and is very, very acc...              0  \n",
              "1      The room was not overly big, but clean and...              1  \n",
              "2      After I plugged it in to my USB hub on my ...              0  \n",
              "3           short battery life  I moved up from a...              0  \n",
              "4      6GHz 533FSB cpu, glossy display, 3, Cell 2...              0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaOQWOWguDYu",
        "outputId": "ccde72e4-c971-484b-e804-a776ac68a83b"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cluster_label=1인 데이터는 호텔로 클러스터링된 데이터임. DataFrame에서 해당 Index를 추출\n",
        "hotel_indexes = document_df[document_df['cluster_label']==1].index\n",
        "print('호텔로 클러스터링 된 문서들의 DataFrame Index:', hotel_indexes)\n",
        "\n",
        "# 호텔로 클러스터링된 데이터 중 첫번째 문서를 추출하여 파일명 표시.  \n",
        "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
        "print('##### 비교 기준 문서명 ',comparison_docname,' 와 타 문서 유사도######')\n",
        "\n",
        "''' document_df에서 추출한 Index 객체를 feature_vect로 입력하여 호텔 클러스터링된 feature_vect 추출 \n",
        "이를 이용하여 호텔로 클러스터링된 문서 중 첫번째 문서와 다른 문서간의 코사인 유사도 측정.'''\n",
        "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]] , feature_vect[hotel_indexes])\n",
        "print(similarity_pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "호텔로 클러스터링 된 문서들의 DataFrame Index: Int64Index([1, 13, 14, 15, 20, 21, 24, 28, 30, 31, 32, 38, 39, 40, 45, 46], dtype='int64')\n",
            "##### 비교 기준 문서명  bathroom_bestwestern_hotel_sfo  와 타 문서 유사도######\n",
            "[[1.         0.05907195 0.05404862 0.03739629 0.06629355 0.06734556\n",
            "  0.04017338 0.13113702 0.41011101 0.3871916  0.57253197 0.10600704\n",
            "  0.13058128 0.1602411  0.05539602 0.05839754]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgrXVe5_uDYu",
        "outputId": "7864a200-aedd-46ac-b064-fc5e52b08b7d"
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# argsort()를 이용하여 앞예제의 첫번째 문서와 타 문서간 유사도가 큰 순으로 \n",
        "# 정렬한 인덱스 반환하되 자기 자신은 제외. \n",
        "sorted_index = similarity_pair.argsort()[:,::-1]\n",
        "sorted_index = sorted_index[:, 1:]\n",
        "\n",
        "# 유사도가 큰 순으로 hotel_indexes를 추출하여 재 정렬. \n",
        "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
        "\n",
        "# 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외\n",
        "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
        "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
        "\n",
        "# 유사도가 큰 순으로 정렬된 Index와 유사도값을 이용하여 파일명과 유사도값을 \n",
        "# Seaborn 막대 그래프로 시각화\n",
        "hotel_1_sim_df = pd.DataFrame()\n",
        "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
        "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
        "\n",
        "sns.barplot(x='similarity', y='filename',data=hotel_1_sim_df)\n",
        "plt.title(comparison_docname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'bathroom_bestwestern_hotel_sfo')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}